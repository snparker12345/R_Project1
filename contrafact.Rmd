---
title: 'GRUPO 6: Pablo Pardo, Álvaro Morán, Sophie Parker'
date: "03/02/2024 Aprendizaje Automático I - Ciencia e ingeniería de datos."
output:
  html_document:
    theme: simplex    
    toc: yes
    toc_float:
      collapsed: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introducción
En esta memoria se detallan los procesos y procedimientos empleados para la resolución de diferentes cuestiones sobre la base de datos "Gender Gap in Spanish Wikipedia". 

Primeramente es conveniente entender el problema a abordar.

## Brecha de género en Wikipedia.
Wikipedia es una enciclopedia de contenido libre que proyecta reunir todo el conocimiento humano. Se ubica en internet y se desarrolla gracias a voluntarios de todo el mundo; esto es, que la redactan y mantienen diversas personas en diferentes ubicaciones del globo. 

Sabemos que la población mundial se encuentra equitativamente dividida entre hombres y mujeres (hay una diferencia insignificante entre ambos grupos). Sin embargo, entre el 84 y el 91 por ciento de quienes editan las páginas de Wikipedia son varones, a pesar de que, como sabemos, son el 50% de la población mundial.

En 2011 la Fundación Wikimedia, organización matriz de Wikipedia sin ánimo de lucro, realizó una encuesta y los resultados fueron verdaderamente significativos:

<center>

![barplot](barplot.svg.png)
</center>\

Puede encontrar más información sobre la brecha de género en Wikipedia en el siguiente enlace:
https://es.wikipedia.org/wiki/Brecha_de_g%C3%A9nero_en_Wikipedia

**NOTA: Específicamente en nuestro problema trabajaremos con datos relativos a los editores de Wikipedia España.**


# Base de datos
La base de datos seleccionada para abordar el problema de disparidad de género en la redacción de Wikipedia España se puede encontrar el subsiguiente enlace: https://archive.ics.uci.edu/dataset/852/gender+gap+in+spanish+wp

Veamos en detalle las variables que componen el dataset:

* Gender: [TARGET] Indica el género codificado. Codificación: 0 para género desconocido, 1 para hombre y 2 para mujer.

* C_api: [TARGET] Indica el género según la API de WikiMedia (véase sección de introducción para comprender la relación de Wikimedia con el dataset en cuestión). Codificación: desconocido, hombre y mujer.

* C_man: [TARGET] Indica el género, el cual se extrae de la codificación del contenido de Wikipedia. Codificación: 1 hombre, 2 mujer, 3 género desconocido.

* E_NEds: Índice i del estrato. Índices: ij. Valores: 0,1,2,3.

* E_Bpag: Índice j del estrato. Índices: ij. Valores: 0,1,2,3.

* firstDay: Fecha de la primera edición en Wikipedia España (YYYYMMDDHHMMSS).

* lastDay: Fecha de la última edición en Wikipedia España (YYYYMMDDHHMMSS).

* NEds: Número total de ediciones.

* NDays: Número de días totales (último día-primer día +1)

* NActDays: Número de días con ediciones.

* NPages: Número de páginas diferentes editadas.

* NPcreated: Número de páginas creadas.

* pagesWomen: Número de ediciones en páginas relacionadas con la mujer.

* wikiprojWomen: Número de ediciones en proyectos 'Wiki' relacionados con la mujer.

* ns_user: Número de ediciones con namespace 'usuario'.

* ns_wikipedia: Número de ediciones con namespace 'Wikipedia'.

* ns_talk: Número de ediciones con namespace 'talk'.

* ns_userTalk: Número de ediciones con namespace 'usertalk'.

* ns_content: Número de ediciones con namespace 'pages'.

* wightIJ: Corrección de peso para estrato ij. 

* NIJ: Número de elementos en el estrato ij.


Nótese que el problema a resolver es de clasificación. Por tanto debemos convertir nuestra variable target a tipo binario. 


# Business understanding. 
Una vez hemos detectado las diferentes variables con las que vamos a trabajar, es fundamental comprender cómo estas se relacionan, entre qué valores se definen, si alguna presenta algún dato atípico...

Realizamos varias preguntas para intentar entender el problema y los valores de los datos contenidos:

¿Se realizan muchas ediciones en Wikipedia?
¿Cuántos proyectos 'Wiki' se realizan en total?
¿Cuántos días se suele tardar en realizar una edición?
¿Por qué existen diferentes 'namespace' para editar Wikipedia?

**OBJETIVO: Predecir el género del editor de una página de Wikipedia dada.**

# Data understanding.
Procedemos a cargar nuestros datos. Escogemos un único target: gender. Por tanto, eliminamos el resto de variales target (C_api y C_man), las cuales contienen datos similares. Además, eliminamos los datos categorizados como 'unknown' del target, reduciendo los niveles de nuestro factor en una unidad, de 3 a 2: 1 hombre, 2 mujer. 

# Librerías.
```{r LIBREÍAS, echo=FALSE, message=FALSE}
library(caret)
library(ggplot2)
library(readr)
library(tidyverse)
library(viridis)
library(dplyr)
library(gridExtra)
library(ggplot2)
library(dplyr)
library(pdp) #pima dataset
library(corrplot)
library(plotly)
library(car)
library(GGally)
library(MASS)
library(modeest)
library(NbClust)
library(cluster)
library(parameters)
library(stats)
library(factoextra)
library(rpart)
library(rpart.plot)
library(randomForest)
library(naivebayes)
library(pROC)
library(xgboost)
library(purrr)
library(Metrics)
library(DALEX)
library(counterfactuals)
library(iml)
```


# Lectura de datos.
```{r lectura, warning=FALSE} 

#Semilla para que nuestro análisis sea reproducible
set.seed(123)

#Lectura de datos en csv
orig_df<-read_csv("datos/data.csv", show_col_types=FALSE)
#Eliminamos los datos con valor 'unknown' en la variable gender.
df_no_unknown<-orig_df[orig_df$gender !=0,]
#Eliminamos las variables target que no vamos a usar
df<-subset(df_no_unknown, select=-c(C_api,C_man))

#Obtenemos la dimensión de la matriz con los que trabajaremos de ahora en adelante
dim(df)
n<- dim(df)[1]  # Así nombraremos las filas
p <- dim(df)[2] # Así nombraremos las columnas
```
Por tanto, tenemos  $n=$```r n```observaciones y $p=$```r p```variables en la base de datos. 


# Division de los datos: train, test y validation.

Para este proyecto, se emplea una proporción de datos del **60%, 20%, 20%** para el entrenamiento, la prueba y la validación respectivamente. Para realizar la partición de los datos se ha empleado la función **createDataPartition** de la **librería caret.**

* Obtenemos nuestros datos de entrenamiento:
```{r DIVISION1, warning=FALSE} 

#Obtengo los índices para el 60% de los datos totales
indices_60 <- createDataPartition(df$gender, times = 1, p = 0.6)

#Se utiliza el 60% de los datos para entrenar el modelo
train_Data <- df[indices_60$Resample1,] # Selecciono todas las columnas
```

Dimensión de datos para el entrenamiento: ```r dim(train_Data)```

* Agrupamos los datos restantes para posteriormente volver a particionar.

```{r DIVISION2, warning=FALSE} 
#El signo menos (-) indica "no". Selecciono los datos que no he seleccionado antes: el 40% restante
data_test_val <- df[-indices_60$Resample1,] # Datos para dividir en prueba y validación

#Obtengo los índices para el 20% de los datos totales (50% de data_test_val)
indices_40 <- createDataPartition(data_test_val$gender, times = 1, p = 0.5)
```

* Obtenemos nuestros datos de prueba:
```{r DIVISION3, warning=FALSE} 

#El 20% de los datos se utiliza para probar el modelo.
test_Data <- data_test_val[indices_40$Resample1,] # Selecciono todas las columnas
```

Dimensión de datos para el test: ```r dim(test_Data)```

* Obtenemos nuestros datos de validación:
```{r DIVISION4, warning=FALSE} 

#Selecciono los datos que no he seleccionado antes: el 50% restante (20% del total)
val_Data <- data_test_val[-indices_40$Resample1,] # Selecciono todas las columnas
dim(val_Data)
```

Dimensión de datos para la validación : ```r dim(val_Data)```

* La suma de particiones debe ser igual a la dimensión total de los datos
```{r COMPR, warning=FALSE} 

#Verifico si las dimensiones coinciden. Objetivo: 3145 observaciones.
dim(train_Data) + dim(test_Data) + dim(val_Data)

```

Definimos como factores en R las variables **gender, E_NEds, E_Bpag, weightIJ y NIJ.**
```{r FACTORS, warning=FALSE} 
train_Data$gender<-as.factor(train_Data$gender)
train_Data$E_NEds <-as.factor(train_Data$E_NEds)
train_Data$E_Bpag <-as.factor(train_Data$E_Bpag)
train_Data$weightIJ<-as.factor(train_Data$weightIJ)
train_Data$NIJ<-as.factor(train_Data$NIJ)
```


# Análisis exploratorio de Datos

## Funciones de representación.

Para facilitar la lectura del código, se han definido diversas funciones de representación:
```{r FUNCIONES DE LOS DATOS, warning=FALSE} 

histogram = function(x_var, title= 'Histograma', x_label, y_label = "Frecuencia") {
  ggplot(train_Data, aes(x = {{x_var}})) +
    geom_histogram(bins = 40, color = 'black', fill = 'white') +
    geom_vline(aes(xintercept = mean({{x_var}})), color = "blue", linetype = "dashed", size = 1) +
    labs(title = title,
         x = x_label,
         y = y_label)
}
pdf = function(x_var, title='PDF', x_label, y_label = "Densidad") {
  ggplot(train_Data, aes(x = {{x_var}})) +
    geom_density() +
    labs(title = title,
         x = x_label,
         y = y_label)
}

boxplot = function(x_var, title = "Boxplot", x_label) {
  ggplot(train_Data, aes({{x_var}})) +
    geom_boxplot() +
    labs(title = title,
         x = x_label)
}
```


## [TARGET] Gender
Observemos nuestra variable gender. La cual tratamos como factor: 1 hombre, 2 mujer.
```{r TARGET, warning=FALSE} 

# Veamos la proporción de hombres y mujeres.
prop.table(table(train_Data$gender))

# Representamos un diagrama de barras.
ggplot(data = train_Data, aes(x = gender, fill = gender)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Frecuencia relativa") +
  xlab("Género")

```
</center>\

## E_NEds, E_Bpag, weightIJ y NIJ
Las siguientes variables se tratarán juntas ya que pertenecen a una discretización de la variable continua (NEds).
```{r}
#Resumen E_NEds
summary(train_Data$E_NEds)
```

```{r}
#Resumen E_Bpag
summary(train_Data$E_Bpag)
```
*En ambas variables se observan 4 grupos.*
```{r}
#Resumen de weightIJ.
summary(train_Data$weightIJ)
```
*Apreciamos que hay 16 grupos distintos.*

```{r}
#Resumen de NIJ
summary(train_Data$NIJ)
```
*Hay 16 grupos al igual que en weightIJ. Las variables E_Bpag y E_NEds están relacionadas dos a dos, las variables NIJ y weightIJ también están relacionadas dos a dos.*

Representemos E_NEds
```{r}
#Histograma para ver como están repartidos los grupos E_NEds.
ggplot(data=train_Data,aes(x=E_NEds,fill=E_NEds)) +
  geom_bar(aes(y=(..count..)/sum(..count..))) +
  scale_y_continuous(labels=scales::percent) +
  theme(legend.position="none") +
  ylab("Frecuencia relativa") +
  xlab("E_NEds")
```
</center>\
*El número de personas en cada grupo está equilibrado.*

Representemos E_Bpag
```{r}
#Histograma para ver como están repartidos los grupos en E_Bpag.
ggplot(data=train_Data,aes(x=E_Bpag,fill=E_Bpag)) +
  geom_bar(aes(y=(..count..)/sum(..count..))) +
  scale_y_continuous(labels=scales::percent) +
  theme(legend.position="none") +
  ylab("Frecuencia relativa") +
  xlab("E_Bpag")
```
</center>\
*En este caso los grupos no tienen una cantidad parecida de integrantes.*

Representemos weightIJ
```{r}
#Histograma para ver como están repartidos los grupos en weightIJ.

ggplot(data=train_Data,aes(x=weightIJ,fill=weightIJ)) +
  geom_bar(aes(y=(..count..)/sum(..count..))) +
  scale_y_continuous(labels=scales::percent) +
  scale_x_discrete(labels = NULL) + 
  theme(legend.position="none") +
  ylab("Frecuencia relativa") +
  xlab("weightIJ")
```
</center>\

Representemos NIJ
```{r}
#Histograma para ver como están repartidos los grupos en NIJ.
ggplot(data=train_Data,aes(x=weightIJ,fill=weightIJ)) +
  geom_bar(aes(y=(..count..)/sum(..count..))) +
  scale_y_continuous(labels=scales::percent) +
  scale_x_discrete(labels = NULL) + 
  theme(legend.position="none") +
  ylab("Frecuencia relativa") +
  xlab("NIJ")
```
</center>\
*Los grupos al igual que en weightIJ vuelven a no estar uniformemente repartidos.*

## firstDay
Fragmentamos la variable en 3 para una mejor comprensión. Observamos independientemente el primer año de edicion, el primer mes y el primer día.

## Año inicial
```{r}
#Descomponemos la fecha.
ano_inicial<-as.numeric(substr(train_Data$firstDay,start = 1, stop = 4 ))
train_Data$ano_inicial<-ano_inicial
```

```{r}
#Resumen de la variable
summary(ano_inicial)
```
*La media coincide con la mediana. El primer y tercer cuartil no presentan valores que puedan indicar una gran asimetría.*

```{r, warning= FALSE}
#Vemos su distribucion con un diagrama de barras

ggplot(data = train_Data, aes(x = ano_inicial, fill = ano_inicial)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Frecuencia relativa") +
  xlab("Año")
```
</center>\
*Se pueden apreciar dos poblaciones, una concentrada en 2006 y otra en 2012.*

```{r}
#Año inicial - PDF
pdf(ano_inicial, x_label="Año")
```
</center>\
*Con la PDF no se consiguen distinguir dos poblaciones.*

```{r}
#Año incial - Boxplot
boxplot(ano_inicial,title = "Año inicial", x_label = "Año")
```
</center>\
*No se observan datos atípicos.*
*El año incial puede ser de interés, ya que como veremos ahora, el mes y el día no presentan nada destacable.*

## Mes inicial
```{r}
#Obtenemos el mes del dato.
mes_inicial<-as.numeric(substr(train_Data$firstDay,start = 5, stop = 6 ))
train_Data$mes_inicial<-mes_inicial


#Resumen del mes inicial.
summary(mes_inicial)
```
*La mediana está muy cerca de la media. El primer y tercer cuartil no presentan valores que puedan indicar una gran asimetría.*

```{r, warning=FALSE}
#Vemos su distribucion con un diagrama de barras

ggplot(data = train_Data, aes(x = mes_inicial, fill = mes_inicial)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Frecuencia relativa") +
  xlab("Mes") + scale_x_continuous(breaks = 1:12, labels = month.abb)
```
</center>\
*El número de cuentas creadas a lo largo del año es regular. El mes de Agosto es en el que mas nuevas cuentas se han creado.*

```{r}
#Boxplot - Mes inicial
boxplot(mes_inicial,x_label = "Mes inicial")
```
</center>\
*El boxplot no muestra valores atípicos y observamos que en los datos no hay apenas variacion.*

## Día inicial
```{r}
#Obtenemos el día.
dia_inicial<-as.numeric(substr(train_Data$firstDay,start = 7, stop = 8 ))
train_Data$dia_inicial<-dia_inicial

#Resumen del dato.
summary(dia_inicial)
```
*La media y la mediana son parecidas. Todos los datos son coherentes con los días del mes. No parece haber errores.*

```{r, warning=FALSE}
#Vemos su distribucion con un diagrama de barras

ggplot(data = train_Data, aes(x = dia_inicial, fill = dia_inicial)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Frecuencia relativa") +
  xlab("Día") + scale_x_continuous(breaks = seq(1, 31, by = 2))
```
</center>\
*Los datos no provienen de una distribucion normal. Cabe destacar que el día 31 no parece tener una menor cantidad de primeras ediciones a pesar de que no todos los meses cuentan con 31 días.*

```{r}
#Día inicial - Boxplot
boxplot(dia_inicial, x_label = "Dia inicial")
```
</center>\
*Nada destacable. Como hemos podido observar, el único dato que parece poder resultar de interés es el año.*

## lastDay
Al igual que con firstDay, hemos dividido esta variable en tres datos, siendo estos el año, el mes y el día.

## Año final
```{r}
#Obtenemos el año final del dato.
ano_final<-as.numeric(substr(train_Data$lastDay,start = 1, stop = 4 ))
train_Data$ano_final<-ano_final
#Resumen del dato.
summary(ano_final)
```
*La media vuelve a coincidir con la mediana, aunque esta vez el tercer cuartil es elevado (2017, el dato más alto), lo que nos puede indicar una asimetría hacia la izquierda, con una larga cola.*

```{r, warning=FALSE}
#Vemos su distribucion con un diagrama de barras

ggplot(data = train_Data, aes(x = ano_final, fill = ano_final)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Frecuencia relativa") +
  xlab("Año")
```
</center>\
*Observamos que los primeros años son bastante uniformes, solo se aprecia un aumento leve, pero en el 2017 se duplican la cantidad de últimas ediciones con respecto al año anterior. A diferencia del año inicial, en este caso no hay sospechas de haber dos poblaciones.*

```{r}
#Año final - Boxplot
boxplot(ano_final, x_label = "Año")
```
</center>\
*La mayoría de los datos están concentrados en últimos años.*

## Mes final
```{r}
#Obtenemos el mes final.
mes_final<-as.numeric(substr(train_Data$lastDay,start = 5, stop = 6 ))
train_Data$mes_final<-mes_final
#Resumen del dato.
summary(mes_final)
```
*Se vuelve a apreciar que los cuartiles son valores estandar, y que la media coincide con la mediana por lo que no se espera nada extraño a la hora de representar los datos.*

```{r, warning=FALSE}
#Vemos su distribucion con un diagrama de barras

ggplot(data = train_Data, aes(x = mes_final, fill = mes_final)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Frecuencia relativa") +
  xlab("Mes") + scale_x_continuous(breaks = 1:12, labels = month.abb)
```
</center>\
*Pese a la suposicion sacada tras ver el resumen de la variable, hay una gran acumulación de últimas ediciones en el mes de Septiembre, muy notable con respecto al resto.*

```{r}
#Mes final - Boxplot
boxplot(mes_final, x_label = "Mes final")
```
</center>\
*No hay datos atípicos*

## Día final
```{r}
#Sacamos el día.
dia_final<-as.numeric(substr(train_Data$lastDay,start = 7, stop = 8 ))
train_Data$dia_final<-dia_final
#Resumen del dato
summary(dia_final)
```
*Vuelven a asemejarse la media y mediana. Los datos en este caso también parecen coherentes.*

```{r, warning=FALSE}
#Vemos su distribucion con un diagrama de barras

ggplot(data = train_Data, aes(x = dia_final, fill = dia_final)) +
  geom_bar(aes(y = (..count..)/sum(..count..))) +
  scale_y_continuous(labels = scales::percent) +
  theme(legend.position = "none") +
  ylab("Frecuencia relativa") +
  xlab("Día") + scale_x_continuous(breaks = seq(1, 31, by = 2))
```
</center>\
*Se observa una concentración de últimas ediciones en el primer día del mes. Y que el día con menos cantidad es el 31, como es de esperar.*

```{r}
#Dia final - Boxplot
boxplot(dia_final, x_label= "Dia")
```
</center>\
*No se observan valores atípicos ni nada destacable.*


## NEds
```{r}
#Resumen de los datos
summary(train_Data$NEds)
```
*Observamos que el primer cuartil es bajo, por lo que tenemos una gran concentración de datos con valores bajos. La media es muy superior a la mediana, lo que va a presentar asimetría a la derecha. También se observa que el máximo de ediciones es (101022), dato que aparentemente parece erróneo.*

```{r}
#Veamos el histograma.
histogram(NEds,x_label = "Cantidad de ediciones", y_label = "Frecuencia",title = "Número de ediciones" )
```
</center>\

```{r}
#NEds - pdf
pdf(NEds, x_label = "Cantidad ediciones")
```
</center>\
*Se observa que la probabilidad se encuentra concentrada en valores bajos*

```{r}
boxplot(NEds,title = "Número de ediciones", x_label = "Frecuencia")
```
</center>\
*Se observa que el valor máximo está muy alejado de los otros valores atípicos, lo que apoya el pensamiento de que puede ser un dato erróneo.*

```{r}
#log(NEds) - Boxplot
boxplot(log(NEds),title = "Número de ediciones (log)", x_label = "Frecuencia")
```
</center>\
*Realizando un boxplot al logaritmo de la variable se aprecian mejor los valores atípicos. Hay una gran cantidad de ellos.*


## NDays.
```{r NDAys1, warning=FALSE} 
# Resumen de la variable NDays en train_Data.
summary(train_Data$NDays)
```
*El primer cuartil toma un valor bajo (814.5) por lo que tenemos una apreciable concentración de datos con valores bajos. Además, la media es algo superior a la mediana, i.e, la distribución presentará una ligera asimetría a la derecha.*

```{r NDAys2, warning=FALSE} 
# Veamos la forma de su distribución con un histograma. La línea azul representa la media.
NDays_hist<-histogram(NDays, x_label= "Último día-Primer día +1"); NDays_hist
```
</center>\
*Los datos no provienen de una distribución normal.*

```{r NDAys3, warning=FALSE} 
# Veamos su función de densidad de probabilidad.
NDays_pdf<- pdf(NDays, x_label='Último día-Primer día +1'); NDays_pdf
```
</center>\
*La probabilidad se encuentra homogéneamente distribuida, aunque hay una gran concentración, como esperábamos, entre los valores 0 y 1500.*

```{r NDAys4, warning=FALSE} 
# Boxplot.
bp_NDays<-boxplot(NDays, x_label='Último día-Primer día +1'); bp_NDays
```
</center>\
*No se observan datos atípicos*.

## NActDays, NPages y NPcreated.
Estudiaremos en conjunto estas variables al presentar una distribución similar. Se obtienen conclusiones similares de todas ellas.

```{r NActDAys0, warning=FALSE} 
# Resumen de la variable NActDays en train_Data. 
summary(train_Data$NActDays)
```
*El primer cuartil toma un valor bajo (24), observamos un máximo de 3346 y una media de 53. Por tanto, tenemos una notoria concentración de datos con valores bajos. Además, la media es bastante superior a la mediana, i.e, la distribución presentará una notable asimetría a la derecha.*

```{r NActDAys1, warning=FALSE}

#NActDAys - Estimación unimodal
mlv(train_Data$NActDays, method = "meanshift")
```
*La moda estimada tiene un valor de 27.19 lo que ratifica nuestra sospecha de que la distribución tendrá una larga cola derecha.*

```{r NActDAys2, warning=FALSE} 

# Resumen de la variable NPages en train_Data. 
summary(train_Data$NPages)
```
*Conclusiones similares a las de la variable NActDays.*

```{r NActDAys3, warning=FALSE} 

# Resumen de la variable NPcreated en train_Data. 
summary(train_Data$NPcreated)
```
*En este caso encontramos valores más bajos, aunque, al igual que las otras dos variables, la distribución tendrá una gran asimetría a la derecha al concentrarse los datos en los valores bajos (Q1= 1.0).*

Observemos sus distribuciones.
```{r NActDAys4, warning=FALSE} 
# NActDays - Veamos la forma de su distribución con un histograma
NActDays_hist<-histogram(NActDays, x_label='Días con ediciones')

# NPages - Veamos la forma de su distribución con un histograma
NPages_hist<-histogram(NPages, x_label='Páginas diferentes editadas')

# NPcreated - Veamos la forma de su distribución con un histograma 
NPcreated_hist<-histogram(NPcreated, x_label='Páginas creadas')

# Organizamos los gráficos en fila
grid.arrange(NActDays_hist, NPages_hist, NPcreated_hist, nrow=1)
```
</center>\
*Como se puede observar, todas las distribuciones presentan baja uniformidad y una notable asimetría a la derecha.*

```{r NActDAys5, warning=FALSE} 

# NActDays - Veamos su función de densidad de probabilidad.
NActDays_pdf<- pdf(NActDays, x_label='Días con ediciones')

# NPages - Veamos su función de densidad de probabilidad.
NPages_pdf<-pdf(NPages, x_label='Páginas diferentes editadas')

# NPcreated - Veamos su función de densidad de probabilidad.
NPcreated_pdf<-pdf(NPcreated, x_label='Páginas creadas')

# Organizamos los gráficos en fila
grid.arrange(NActDays_pdf, NPages_pdf, NPcreated_pdf, nrow=1)
```
</center>\
*La probabilidad siempre se encuentra concentrada en los valores bajos. Esto es esperado, considerando los datos analizados anteriormente.*

```{r NActDAys6, warning=FALSE} 

# NActDays - Diagrama de caja.
bp_NActDays<-boxplot(NActDays, x_label='Días con ediciones'); bp_NActDays
```
</center>\
*Observamos muchos datos superiores a Q3 + 1.5RI. Sin embargo, solo se considerarían datos atípicos aquellos con valor superior a 3000 aproximadamente.*

```{r NActDAys7, warning=FALSE} 
# NPages - Diagrama de caja.
bp_NPages<-boxplot(NPages, x_label='Páginas diferentes editadas'); bp_NPages
```
</center>\
*Observamos muchos datos superiores a Q3 + 1.5RI. Sin embargo, solo se considerarían datos atípicos aquellos con valor superior a 40000 aproximadamente.*

```{r NActDAys8, warning=FALSE} 
# NPcreated - Diagrama de caja.
bp_NPcreated<-boxplot(NPcreated, x_label='Páginas creadas'); bp_NPcreated
```
</center>\

*Observamos un par de datos atípicos a partir del valor 2000. Especialmente, cabe destacar el valor máximo de la función, el cual es superior a 6000.*

*Esta variable indica el número de páginas creadas. Puede haber sido dato introducido erróneamente, aunque podría  ser verídico, y no parece pertenecer a ninguna de las variables cercanas, i.e NPages o pagesWomen.*

Transformemos los datos para poder trabajar con una distribución con mejores propiedades. **Véase que NPcreated, al ser una variable dispersa, no acepta una transformación box cox.**

```{r NActDAys9, warning=FALSE} 

# NActDays - Transformación de datos (box cox).
boxcox(lm(train_Data$NActDays ~ 1)) 
```
</center>\
Obtenemos el valor de lambda=0 --> TRANSFORMACIÓN: log10(x). **Esta transformación también será la empleada para los datos de Npages.**

**Véase que se suma 0.5 a la transformación debido a que presenta valores nulos.** Al realizar la transformación logarítmica se obtendría un valor -inf, lo cual imposibilitaría el desarrollo de posteriores tratamientos de datos. 

Esta inocua modificación para no obtener indeterminaciones matemáticas también se aplica a la variable NPages y a otras variables posteriores.

```{r NActDAys10, warning=FALSE} 
new_NActDays<- log10(train_Data$NActDays+0.5)
#Representamos el histograma con los datos transformados
histogram(new_NActDays, x_label='Días con ediciones')
```
</center>\

```{r NActDAys11, warning=FALSE} 
new_NPages<- log10(train_Data$NPages+0.5)
#Representamos el histograma con los datos transformados
histogram(new_NPages, x_label='Páginas diferentes editadas')
```
</center>\

## pagesWomen
```{r pagesWomen1, warning=FALSE} 
# Resumen de la variable pagesWomen en train_Data.
summary(train_Data$pagesWomen)
```
</center>\
*Podemos apreciar que, tanto el primer como el tercer cuartil, tienen un valor de 0. Además, la mediana es 0 y la media está muy cercana a 0.Por tanto, estamos ante una **sparse variable**, puesto que la mayoría de valores son 0. *

```{r pagesWomen2, warning=FALSE} 
# Veamos la forma de su distribución con un histograma 
pagesWomen_hist<-histogram(pagesWomen, x_label='Ediciones en pág. Mujer'); pagesWomen_hist
```
</center>\
*Confirmamos nuestra conclusión anterior.*

```{r pagesWomen3, warning=FALSE} 
# Veamos su función de densidad de probabilidad.
pagesWomen_pdf<-pdf(pagesWomen, x_label='Ediciones en pág. Mujer'); pagesWomen_pdf
```
</center>\
*La probabilidad se encuentra únicamente concentrada en los valores bajos. Esto es esperado, considerando los datos del summary analizado anteriormente.*

```{r pagesWomen4, warning=FALSE} 
# Diagrama de caja.
bp_pagesWomen<-boxplot(pagesWomen, x_label='Ediciones en pág. Mujer'); bp_pagesWomen
```
</center>\
*Encontramos un par de datos atípicos entre los valores 50 y 100, aunque el valor atípico más destacable se encuentra totalmente aislado por encima del valor 150. Este dato es el máximo que se indica en el summary: 185.*

*Teniendo en cuenta que se trata de una variable que incluye información sobre el número de ediciones en páginas relacionadas con la mujer, parece un error.* 

*Otra opción sería considerar que ese dato se encuentra ahí debido a un error humano y que realmente pertenece a otra variable. En el rango de la variable anterior a pagesWomen, i.e NPcreated, un valor de 185 podría encajar (en ningún caso 185000) pero NPcreated no tiene ningún valor faltante. Esto nos hace pensar que no acepta ningún valor añadido.*

*Por otro lado, la siguiente variable a pagesWomen, i.e wikiprojWomen, tiene un comportamiento muy similar a pagesWomen, por lo que en ningún caso aceptaría ese dato.*

*Finalmente, optamos por binarizar la varibale, dado que la gran mayoría de los datos toman valor 0.*

```{r}
#Binarización de pagesWomen
train_Data$pagesWomen_bin<-ifelse(train_Data$pagesWomen>0,1,0)
table(train_Data$pagesWomen_bin)
```
*Observamos que el 95% de los datos toman valor distinto de 0.*

```{r}
#Histograma de la variable.
ggplot(data=train_Data,aes(x=factor(pagesWomen_bin),fill=pagesWomen_bin)) +
  geom_bar(aes(y=(..count..)/sum(..count..))) +
  scale_y_continuous(labels=scales::percent) +
  theme(legend.position="none") +
  ylab("Frecuencia relativa") +
  xlab("Variable respuesta: Grupo")
```
</center>\

## wikiprojWomen
```{r wikiprojWomen, warning=FALSE} 
# Resumen de la variable wikiprojWomen en train_Data.
summary(train_Data$wikiprojWomen)
```
</center>\
*Prácticamente todos los datos toman el valor 0 considerando que Q3 =0. La media no es 0 porque hay un valor máximo muy elevado.*

```{r}
#Histograma - wikiprojWomen
histogram(wikiprojWomen, x_label = "Ediciones")
```
</center>\
*Nos confirma que la mayoria de los valores son 0 a excepción de unos pocos.*

```{r}
#wikiprojWomen - PDF
pdf(wikiprojWomen, x_label = "Ediciones")
```
</center>\
*La probabilidad está concentrada en el 0.*

```{r}
#wikiprojWomen - Boxplot
boxplot(wikiprojWomen, x_label = "Ediciones")
```
</center>\
*Se observa algunos datos superiores a Q3 + 1.5RI. Solo se encuentra un único dato atípico. Aunque el valor máximo difiera mucho del segundo valor más alto (valor máximo = 762, segundo valor max = 20) no podemos asumir que se trate de un dato erróneo.*

*Tras este analisis, binarizaremos la variable, donde cualquier dato que tome valor 0 será 0, y cualquiera distinto de 0 será 1.*
```{r}
#Binzarizamos variable
train_Data$wikiprojWomen_bin<-ifelse(train_Data$wikiprojWomen>0,1,0)
```

```{r}
#Resumen de la nueva variable.
table(train_Data$wikiprojWomen_bin)
```
*Observamos la poca cantidad de valores distintos de 0 que hay.*

```{r}
#Histograma de la variable.
ggplot(data=train_Data,aes(x=factor(wikiprojWomen_bin),fill=wikiprojWomen_bin)) +
  geom_bar(aes(y=(..count..)/sum(..count..))) +
  scale_y_continuous(labels=scales::percent) +
  theme(legend.position="none") +
  ylab("Frecuencia relativa") +
  xlab("Variable respuesta: Grupo")
```
</center>\


## ns_user
```{r ns_user, warning=FALSE} 
#Resumen de la variable ns_user.
summary(train_Data$ns_user)
```
*El primer cuartil toma un valor bajo (5), hay un máximo de 2989 y una media de 75. Además la media es muy superior a la mediana, por lo que la distribución presentará una gran asimetría en la derecha.*

```{r}
#ns_user - Histograma
histogram(ns_user, x_label = "Número de ediciones")
```
</center>\
*Comprobamos la distribución presenta una notable asimetría a la derecha. Presenta también una baja uniformidad.*

```{r}
#ns_user - PDF
pdf(ns_user,  x_label = "Número de ediciones")
```
</center>\
*Vemos que la probabilidad se encuentra concentrada en los valores bajos, confirmando así lo dicho previamente.*

```{r}
#ns_user - Boxplot
boxplot(ns_user, x_label = "Número ediciones")
```
</center>\
*Se oservan muchos datos superiores a Q3 + 1.5RI. No consideraremos ningún dato como atípico.*
*Transformaremos los datos para trabajar con una distribución con mejores propiedades.*

```{r}
boxcox(lm(train_Data$ns_user ~ 1))
```
</center>\
*Obtenemos un valor de lamba = 0 -> TRANSFORMACIÓN: log10(x).*

```{r}
new_ns_user<-log10(train_Data$ns_user)
#Histograma con los datos transformados.
histogram(new_ns_user,x_label = "Número ediciones")
```
</center>\

## ns_wikipedia
```{r}
#Resumen de ns_wikipedia.
summary(train_Data$ns_wikipedia)
```
*Observamos que el tercer cuartil es extremandamente bajo, por lo que los datos se concentrarán en valores bajos. También hay un máximo muy elevado (8089), algo tener en cuenta.*

*La media toma un valor mucho más alto que la mediana por lo que se apreciará una asimetría a la derecha.*
```{r}
#ns_wikipedia - Histograma
histogram(ns_wikipedia, x_label = "Número de ediciones")
```
</center>\
*Observamos baja uniformidad y confirmamos la asimetría a la derecha.*

```{r}
#ns_wikipedia - PDF
pdf(ns_wikipedia, x_label = "Número de ediciones")
```
</center>\
*La probabilidad se concentra en los valores bajos.*

```{r}
#ns_wikipedia - Boxplot
boxplot(ns_wikipedia, x_label = "Número de ediciones")
```
</center>\
*Observamos bastantes datos superiores a Q3 + 1.5RI. Sin embargo, solo se considerarán atípicos datos con un valor superior a 6000.*

*Transformamos los datos para obtener una distribución mejor con la que trabajar.*
```{r mix, warning=FALSE} 
new_ns_wikipedia<-log10(train_Data$ns_wikipedia +0.5)
#Histograma con los datos transformados.
histogram(new_ns_wikipedia,x_label = "Número ediciones")
```
</center>\

## ns_talk
```{r}
#Resumen de la variable
summary(train_Data$ns_talk)
```
*El primer y tercer cuartil toman valores bajos, por lo que ahí es donde estarán concentrados los datos. La media es muy inferior a la mediana lo que indica que habrá una asimetría a la derecha. Destacar que el valor máximo es muy elevado respecto al resto de los datos.*
```{r}
#ns_talk - Histograma
histogram(ns_talk, x_label = "Número de ediciones")
```
</center>\
*Se observa baja uniformidad y confirmamos la asimetría hacia a la derecha.*
```{r}
#ns_talk - PDF
pdf(ns_talk,x_label = "Número de ediciones")
```
</center>\

*La probabilidad se encuentra en los valores bajos. Era de esperar tras el analisis previo.*
```{r}
#ns_talk - Boxplot
boxplot(ns_talk, x_label = "Número de ediciones")
```
</center>\
*Se observa una gran cantidad de valores que superan Q3 + 1.5RI. Sin embargo, consideraremos atípicos los valores superiores a 2000.*

```{r}
# ns_talk - Transformación de datos (box cox).
boxcox(lm((train_Data$ns_talk)+0.5 ~ 1))
```
</center>\
*Obtenemos un valor de lambda = 0. -> TRANSFORMACIÓN: log10(x)*

```{r}
new_ns_talk<-log10(train_Data$ns_talk + 0.5)
#Histograma con los datos transformados.
histogram(new_ns_talk,x_label = "Número ediciones")
```
</center>\

## ns_userTalk
```{r}
#Resumen de ns_userTalk.
summary(train_Data$ns_userTalk)
```
*El primer y tercer cuartil toman valores muy bajos mientras que el máximo es muy alto en comparación, y al ser la media muy superior a la mediana se espera uan asimetría a la derecha.*

```{r}
#ns_userTalk - Histograma
histogram(ns_userTalk,x_label = "Número de ediciones")
```
</center>\
*Se observa la asimetría  a la derecha y una baja uniformidad. Nos confirma que la gran mayoría de los datos toman valor 0 o cercano.*

```{r}
#ns_userTalk - PDF
pdf(ns_userTalk,x_label = "Número de ediciones")
```
</center>\
*Como era de esperar la probabilidad está concentrada en valores muy bajos.*

```{r}
#ns_userTalk - Boxplot
boxplot(ns_userTalk, x_label = "Número de ediciones")
```
</center>\
*Se oberva una gran catidad de valores superiores a Q3 + 1.5RI. Se considerarán atípicos los valores superiores a 7500.*

*Transformaremos los datos para trabajar con una mejor distribución.*
```{r}
boxcox(lm((train_Data$ns_userTalk + 0.5) ~ 1))
```
</center>\
*Obtenemos un valor de lamba = 0 -> TRANSFORMACIÓN: log10(x).*

```{r}
new_ns_userTalk<-log10(train_Data$ns_userTalk +0.5)
#Histograma con los datos transformados.
histogram(new_ns_userTalk,x_label = "Número ediciones")
```
</center>\

## ns_content
```{r}
#Resumen de ns_content.
summary(train_Data$ns_content)
```
*El primer cuartil toma un valor bajo, obervamos una media de 1510 y un máximo de 86026. Por tanto, tenemos una detacable concetración de valores bajos. Además, la media es muy superior a la mediana, lo que indica que presentará una notable asimetría a la derecha.*

```{r}
#ns_content - Histograma
histogram(ns_content, x_label = "Número de ediciones")
```
</center>\
*Confirmamos que los datos están concentrados en valores bajos, se observa la asimetría a la derecha. La distribución presenta una baja uniformidad.*

```{r}
#ns_content - PDF
pdf(ns_content, x_label = "Número de ediciones")
```
</center>\
*La probabilidad se encuentra concentrada en valores bajos. Esto es esperado tras el analisis previo.*

```{r}
#ns_content - Boxplot
boxplot(ns_content,x_label = "Número de ediciones")
```
</center>\
*Observamos muchos datos superiores a Q3 + 1.5RI. Sin embargo, solo se considerarán atípicos aquellos superiores a 8000.*

*Transformaremos los datos para obtener una mejor distribución con la que trabajar.*

```{r}
# ns_content - Transformación de datos (box cox).
boxcox(lm(train_Data$ns_content + 0.5 ~ 1)) 
```
</center>\
*Obtenemos el valor de lambda=0 –> TRANSFORMACIÓN: log10(x). Esta transformación también será la empleada para los datos de ns_content.*

```{r}
new_ns_content<- log10(train_Data$ns_content +0.5)
#Representamos el histograma con los datos transformados
histogram(new_ns_content, x_label='Número de ediciones')
```
</center>\
*No se tratan de grupos uniformes.*



# Correlación con el target

Observemos qué variables tienen una mayor correlación con nuestra variable target gender. **RECUERDE: En nuestra variable target, codificamos hombre como 1 y mujer como 2.**

Realizaremos el test de la T de Student y de Chi2 para contrastar las gráficas matemáticamente.

Observemos si la diferencia es significativa realizando el test t-Student:

* Hipótesis nula: no hay diferencia significativa entre medias.
* Hipótesis alternativa: hay diferencia significativa entre medias.

Para variables categóricas empleamos el test de Chi^2:

* Hipótesis nula: no hay relación entre la variable x y el target.
* Hipótesis alternativa: existe una relación la variable x y el target.


## NDays
```{r, warning=FALSE}

# PDF cruzada con target
grid.arrange(ggplot(train_Data, aes(x = NDays, colour = train_Data$gender)) +
  geom_density(lwd=2, linetype=1))
```
</center>\
*En el caso de las mujeres, los días totales trabajados se concentran en valores bajos, disminuyendo drásticamente con forme aumentan en número. En el caso de los hombres se observa una PDF más distribuida a lo largo de los diferentes valores observados. Al haber un mayor porcentaje de hombres en la organización, es coherente que en la variable "días totales en los que se ha desarrollado actividad" los hombres jueguen un papel significante.*

```{r}
t.test(NDays ~ train_Data$gender, data = train_Data)

```
*No hay diferencia significativa entre medias.*


## Variables NActDays y NPages

Al trabajar con variables coninuas representamos sus PDFs y realizamos el test de la t de Student.
```{r, warning=FALSE}

# PDF cruzada con target
grid.arrange(ggplot(train_Data, aes(x = new_NActDays, colour = train_Data$gender)) +
  geom_density(lwd=2, linetype=1), ggplot(train_Data, aes(x = new_NPages, colour = train_Data$gender)) +
  geom_density(lwd=2, linetype=1), nrow=1)
```
</center>\
*Los días con ediciones por hombres y páginas diferentes editadas por hombres son algo mayores que sus respectivos femeninos.* 

```{r}
# Test de contraste
t.test(new_NActDays ~ train_Data$gender, data = train_Data)
t.test(new_NPages ~ train_Data$gender, data = train_Data)
```
*Las medias difieren significativamente en ambos casos.*


## Variables PagesWomen_bin y WikiprojWomen_bin

En este caso, al tratarse de variables categóricas, representamos los diagramas de barras y contrastamos con el test de chi-cuadrado.

```{r, warning=FALSE}

# Diagrama de barras cruzado con target
grid.arrange(ggplot(data = train_Data, aes(x = train_Data$pagesWomen_bin, fill = train_Data$gender)) +
    geom_bar(), ggplot(data = train_Data, aes(x = train_Data$wikiprojWomen_bin, fill = train_Data$gender)) +
    geom_bar(), nrow=1)
```
</center>\
*La mayoría de las ediciones en páginas relacionadas con la mujer se debe principalmente a hombres. Sin embargo, la mayoría de las ediciones en proyectos relacionados con la mujer se debe a mujeres.*

*Por otro lado, las páginas/proyectos no relacionadas con la mujer (columnas en 0 en ambos gráficos) están desarrollados principalmente por hombres.*

```{r, warning=FALSE}
# Test de contraste
chisq.test(train_Data$pagesWomen_bin, train_Data$gender)
```
```{r, warning=FALSE}

chisq.test(train_Data$wikiprojWomen_bin, train_Data$gender)
```
*Observamos que en el primer caso no se rechaza la hipótesis nula (alpha=0.05 por defecto), por lo que no hay relación significativa entre pages_Women_bin y el target. Sin embargo, en el caso de wikiprojWomen_bin, sí se observa una clarísima relación, al arrojarse un p-valor prácticamente nulo.*


## FirstDay
De la variable firstDay, solo nos es de interés el año inicial como ya hemos visto en el analisis univariante.

```{r}
aggregate(ano_inicial ~ gender, data = train_Data, FUN = mean)
```
*Las medias para el primer año con edición para cada género difieren. Para hombres la media está en 2010 y para las mujeres en 2012.*

```{r}
#Densidades del año de primeras ediciones

ggplot(train_Data, aes(x = ano_inicial, fill = gender)) + 
  geom_density(alpha = 0.5) +
  labs(title = "Densidad del Primer Año de Edición por Género", x = "Primer Año de Edición", y = "Densidad") +
  theme_minimal() +
  scale_fill_manual(values = c("1" = "blue", "2" = "pink"), 
                    labels = c("1" = "Hombre", "2" = "Mujer"))

```
</center>\
*Se pueden distinguir dos poblaciones diferentes, una para cada género.*
```{r}
t.test(ano_inicial ~ train_Data$gender, data = train_Data)
```
*Sale un p-valor inferior a 0.05 por lo que rechazamos la hipótesis nula. Sí existe una relación entre el año inicial y el género.*

## lastDay
```{r}
aggregate(ano_final ~ gender, data=train_Data,FUN = mean)
```
*Para el año final de ediciones observamos que las medias en ambos géneros son prácticamente idénticas.*
```{r}
ggplot(train_Data, aes(x = ano_final, fill = gender)) + 
  geom_density(alpha = 0.5) +
  labs(title = "Densidad del Último Año de Edición por Género", x = "Último Año de Edición", y = "Densidad") +
  theme_minimal() +
  scale_fill_manual(values = c("1" = "blue", "2" = "pink"), 
                    labels = c("1" = "Hombre", "2" = "Mujer"))
```
</center>\
*Para el último año no se observa nada de interés.*
```{r}
t.test(ano_final ~ train_Data$gender, data = train_Data)

```
*Para el año final nos sale un p-valor superior a 0.05, por lo que no rechazamos la hipótesis nula y no podemos concluir que tenga un efecto signficativo en el género. *

## NEds

```{r}
ggplot(train_Data, aes(x = log(NEds), fill = gender)) + 
  geom_density(alpha = 0.5) +
  labs(title = "Densidad Número de Ediciones", x = "Número de ediciones", y = "Densidad") +
  theme_minimal() +
  scale_fill_manual(values = c("1" = "blue", "2" = "pink"), 
                    labels = c("1" = "Hombre", "2" = "Mujer"))
```
</center>\
*Aplicamos logaritmo a la variable para poder trabajar mejor. No se ve una diferencia clara entre los grupos respecto al número de ediciones.*

```{r}
t.test(NEds ~ train_Data$gender, data = train_Data)

```
*Nos da un p-valor elevado, (mayor a 0.05) por lo que no podemos asumir que sea de interés para el género.*

## E_NEds, E_Bpag, NIJ y weightIJ
```{r}


# Graficar las cuatro variables
plot1 <- ggplot(data = train_Data, aes(x = E_NEds, fill = gender)) +
  geom_bar() +
  ggtitle("Variable 1: E_NEds") +
  scale_fill_manual(values = c("1" = "blue", "2" = "pink"), labels = c("Hombre", "Mujer"))

plot2 <- ggplot(data = train_Data, aes(x = E_Bpag, fill = gender)) +
  geom_bar() +
  ggtitle("Variable 2: E_Bpag") +
  scale_fill_manual(values = c("1" = "blue", "2" = "pink"), labels = c("Hombre", "Mujer"))

plot3 <- ggplot(data = train_Data, aes(x = NIJ, fill = gender)) +
  geom_bar() +
  ggtitle("Variable 3: Otra Variable 1") +
  scale_fill_manual(values = c("1" = "blue", "2" = "pink"), labels = c("Hombre", "Mujer")) +
  theme(axis.text.x = element_blank())

plot4 <- ggplot(data = train_Data, aes(x = weightIJ, fill = gender)) +
  geom_bar() +
  ggtitle("Variable 4: Otra Variable 2") +
  scale_fill_manual(values = c("1" = "blue", "2" = "pink"), labels = c("Hombre", "Mujer")) +
  theme(axis.text.x = element_blank())

# Combinar los gráficos
grid.arrange(plot1, plot2, plot3, plot4, nrow = 2, top = "Gráficos de variables")
```
</center>\

Realizando el test de chi^2 con las variables se obtiene lo siguiente:
```{r}
chisq.test(train_Data$E_NEds,train_Data$gender)
```
```{r}
chisq.test(train_Data$E_Bpag,train_Data$gender)
```
```{r}
chisq.test(train_Data$NIJ,train_Data$gender)
```
```{r}
chisq.test(train_Data$weightIJ,train_Data$gender)
```
*Se observa que en todas rechazamos la hipótesis nula porque el p-valor obtenido es menor a 0.05. Pero en E_Bpag,  el p-valor es 0.04, por lo que tiene una menor importancia a la hora de explicar el género.*

## Ns_user y New_ns_userTalk

*NOTA*: Al igual que con *ns_content*, *NAct_days*, *ns_talk* y *ns_wikipedia*, con *Ns_user* y *New_ns_userTalk* conviene trabajar con su versión transformada de forma logarítmica. Esto facilitará la visualización gráfica e interpretación de las mismas.

```{r}
# Graficamos las PDF
grid.arrange(ggplot(train_Data, aes(x = new_ns_user, colour = gender)) + geom_density(lwd=2, linetype=1), 
             ggplot(train_Data, aes(x = new_ns_userTalk, colour = gender)) + geom_density(lwd=2, linetype=1), nrow=1)
```
</center>\
*En relación a el número de ediciones en namespace "user" se puede concluir que existe una igualdad en relación al género de sus editores. En namespce "usertalk" (gráfico derecho), parece que hay más ediciones de hombres, al haber una menor densidad de probabilidad en 0 y comportarse de forma similar que su correspondiente gráfica femenina.*

```{r}
t.test(new_ns_user ~ train_Data$gender, data = train_Data)
```
*No existe una relación importante entre las ediciones en "user" y el género.*

```{r}
t.test(new_ns_userTalk ~ train_Data$gender, data = train_Data)
```
*No existe una relación importante entre las ediciones en "user talk" y el género.*

## Ns_content
Se representa Ns_content junto a NAct_days por su similar PDF. NAct_days ya ha sido analizada individualmente anteriormente.
```{r}
# Se representa Ns_content junto a NAct_days por su similar PDF
grid.arrange(ggplot(train_Data, aes(x = new_ns_content, colour = gender)) + geom_density(lwd=2, linetype=1), ggplot(train_Data, aes(x = new_NActDays, colour = gender)) + geom_density(lwd=2, linetype=1), nrow=1)
```
</center>\
*El número de ediciones en páginas relacionadas con contenido es mayoritariamente femenino en los valores entre 0 y 2, apreciándose en 2 una gran diferencia con respecto a los hombres. Sin embargo, para un número de ediciones mayor a 2, los hombres juegan un papel más importante.*

```{r}
t.test(new_ns_content ~ train_Data$gender, data = train_Data)
```
*Existe una relación fuerte entre la transformación logarítmica de ns_content y el género.*


## Ns_talk y Ns_wikipedia

Grfiquemos su relación con el género.
```{r}
# Ambas variables tienen una distribución similar
grid.arrange(ggplot(train_Data, aes(x = new_ns_talk, colour = gender)) +geom_density(lwd=2, linetype=1), 
             ggplot(train_Data, aes(x = new_ns_wikipedia, colour = gender)) + geom_density(lwd=2, linetype=1), nrow=1)
```
</center>\
*Se aprecia que el número de ediciones en "talk" se desarrolla principalmente por hombres, al tener una mayor densidad en valores medios-altos y una menor densidad en 0. En el caso del número de ediciones en namespace "wikipedia" existe una mayor paridad de género.*

```{r}
t.test(new_ns_talk ~ train_Data$gender, data = train_Data)
```
*Existe una relación fuerte entre la transformación logarítmica de ns_talk y el género.*

```{r}
t.test(new_ns_wikipedia~ train_Data$gender, data = train_Data)
```
*No existe relación alguna entre la transformación logarítmica de ns_wikipedia y el género.*

## CONCLUSIÓN

[NUEVO]

Hemos tenido que separar dos variables de fechas en otras tres distintas cada una. En el análisis univariante hemos visto que las variables no tenían distribuciones ideales, presentaban colas largas y con una gran concentración de valores en 0. A estas variables les hemos aplicado una transformación logarítmica con la que hemos obtenido unas distribucines mas fáciles de manejar. Tambíen las variables con mucha cantidad de 0 y muy pocos distintos de 0 las hemos binarizado. 

Respecto al análisis multivariante, tras hacer las pruebas de t-student para las variables continuas y la de chisq para las categóricas obtenemos que las variables con p-valores bajos (tienen una relación con el género) fueron: NDays, la binarización de la variable wikiprojWomen, las transformaciones logarítmicas de NActdays, NPages, ns_content y ns_talk, E_Neds, NIJ, weightIJ y ano_inicial.


# Técnicas de reducción de la dimensionalidad: PCA

## Análisis de variables factibles para el PCA.
A continuación, realizamos el análisis de componentes principales para reducir la dimensionalidad de nuestra muestra de datos **train_Data**. 

En nuestro dataset encontramos variables continuas y categóricas. Para realizar nuestro PCA solo seleccionamos las variables continuas o discretas con muchos valores. Además, eliminamos variables redundantes, que categorizan otras más generales: E_Neds, E_Bpag, NIJ (estas tres variables explican N_Eds). Asimismo, eliminamos las variables que contienen información sobre chas, expresadas con un único número.

Por otro lado, hay variables que, tal y como se representan originalmente, son difíciles de interpretar. Por tanto, para un mejor entendimiento y posterior análisis del PCA, modificamos primero las variables con una distribución asimétrica y poco descriptiva. Estas variables modificadas sustituirán en nuestro conjunto de datos para el PCA a sus hermanas no modificadas.

**NOTA**. Véase que hay variables que ven sus valores incrementados en 0.5. Esto se realiza para que no contengan ningún valor que sea estrictamente 0. En este caso, la transformación logarítmica otorgaría un valor de -inf, lo cual nos imposibilitaría la realización del PCA.

```{r}
# Transformación de variables
new_NActDays<- log10(train_Data$NActDays+0.5)
new_NPages<- log10(train_Data$NPages)
new_ns_talk<- log10(train_Data$ns_talk+0.5)
new_ns_userTalk<- log10(train_Data$ns_userTalk+0.5)
new_ns_content<- log10(train_Data$ns_content+0.5)
new_ns_user<- log10(train_Data$ns_user)
new_ns_wikipedia<- log10(train_Data$ns_wikipedia+0.5)
```

Ahora eliminemos las variables que no introduciremos al PCA y añadimos las variables modificadas anteriormente. Así obtenemos nuestro vector de datos **train_PCA**.

```{r}
# Eliminamos las variables que no podemos introducir al PCA
NEds<-train_Data$NEds
NPcreated<-train_Data$NPcreated
NDays<-train_Data$NDays
delete_PCA <- subset(train_Data, select=-c(gender,E_NEds, E_Bpag, firstDay, lastDay, NIJ,weightIJ,NActDays,NPages,ns_talk,ns_userTalk,ns_content,ns_user, ns_wikipedia,wikiprojWomen,pagesWomen,ano_inicial,mes_inicial,dia_inicial,ano_final,mes_final,dia_final))

# Añadimos variables a introducir en el PCA
train_PCA<- cbind(delete_PCA,new_NActDays, new_NPages,new_ns_talk, new_ns_userTalk, 
                  new_ns_content,new_ns_user,new_ns_wikipedia)
dim(train_PCA)
```
Finalmente, introduciremos ```r dim(train_PCA)[2] ``` variables a nuestro PCA.


## Principal Component Analysis.
Buscamos entender los datos medainte agrupaciones en diferentes componentes principales. Para ello, realizamos el PCA con nuestros datos estandarizados.

```{r}
PCA_std <- prcomp(train_PCA, scale=T); PCA_std
```
Analicemos las primeras componentes principales obtenidas:

* PC1 asigna pesos positivos a todas las variables, representando un promedio de las mismas.

* PC2 compara el numero de ediciones en proyectos wiki de mujeres y las ediciones en las páginas relacionadas con mujeres con el resto de variables.

* PC3 compara el número de páginas creadas con número de ediciones con namespace 'usuario'.


Observemos el resumen del PCA.
```{r}
summary(PCA_std)
```
Con las 2 primeras componentes recogemos el 60% de la variabilidad. Con 4 recogeríamos un 75%. En nuestro caso, trabajaremos con las dos primeras componentes, puesto que las siguientes no añaden de forma significativa información adicional. 

Veamos representada la varianza de cada componente:

```{r}
plot(PCA_std)
```
</center>\
*En efecto, ratificamos que la primera componente principal acumula la mayor parte de la varianza.*

Otra interpretación podría ser la selección de las 4 primeras componentes principales, aportando estas dos últimas una explicabilidad promedio del 8% cada una. En este caso, se vería representado un 79% de de la varibilidad total, pero la complejidad de la interpretación incrementaría notoriamente y posiblemente podríamos encontrar información reduntante.

Representemos nuestras componentes 1 y 2 cruzadas con el target.
```{r}
plot(PCA_std$x[,1], PCA_std$x[,2], col=train_Data$gender)
```
*No podemos distinguir regiones claras donde se observe una concentración de datos con gender=mujer. Si bien la mayor parte de los datos se agrupa en valores bajos, i.e la mayoría de las mujeres se encuentran en valores cercanos a 0 en la PC1 y PC2, esta información no es significante.*

Crucemos las variables con mayor variabilidad en PC1: **new_NActDays** y **new_NPages**. Coloreamos por el target.
```{r}
plot(train_PCA$new_NActDays, train_PCA$new_NPages, col=train_Data$gender)
```
</center>\
*Se observa que las variables presentan una relación lineal.*

Crucemos ahora las variables que tienen pesos altos y opuestos en PC2: NDays y wikiprojWomen_bin.
```{r}
plot(train_PCA$wikiprojWomen_bin, train_PCA$NDays, col=train_Data$gender)
```
</center>\
*Parece que las personas que han trabajado en algún proyecto relacionado con la mujer han sido principalmente mujeres (columna derecha). En cambio, la mayoría de personas que no ha trabajado en este tipo de proyectos han sido hombres principalmente. Además, los días empleados en los proyectos relacionados con la mujer han sido relativamente pocos, puesto que los datos se concentran entre los valores 0 y 2500.*


## Conclusión

*Seleccionamos las primeras dos componentes principales, aunque no conseguimos diferenciar un agrupamiento por género al enfrentarlas. Lo que sí nos ha permitido el PCA es poder detectar diversas variables que no serán importantes a la hora de entrenar modelos posteriormente.* 



# Modelos de Aprendizaje Automático

## ML no supervisado: K-MEANS
Una vez realizado el PCA, buscamos diferenciar grupos ocultos en nuestro dataset. Para ello, empleamos diferentes procedimientos que nos permiten esablecer el número óptimo de grupos y poder visualizar qué datos se parecen y cómo se agrupan. 

Al igual que con el PCA, debemos seleccionar las variables con las que vamos a trabajar. Emplearemos el mismo conjunto de datos que para el PCA, donde se han eliminado variables discretas, fechas y se han transformado otras variables. Además, escalamos nuestras variables.

```{r}
#Eliminamos las variables que no podemos usar
delete_Kmeans <- subset(train_Data, select=-c(gender,E_NEds, E_Bpag, firstDay, lastDay, 
                                           NIJ, weightIJ,NActDays,NPages,ns_talk,ns_userTalk,
                                           ns_content,ns_user, ns_wikipedia,wikiprojWomen,pagesWomen,ano_inicial,ano_final,mes_inicial,dia_inicial,dia_final,mes_final))

# Subconjunto de datos
train_Kmeans<- cbind(delete_PCA,new_NActDays, new_NPages,new_ns_talk, new_ns_userTalk, 
                  new_ns_content,new_ns_user,new_ns_wikipedia)

#Estandarizamos las variables
train_Kmeans_std <- scale(train_Kmeans)

```



### Método del codo.

Buscamos obtener el número de clusters óptimo para la división de nuestros datos.

```{r}
# Método del codo
fviz_nbclust(train_Kmeans_std, kmeans, method = "wss")
```
</center>\
*Una vez analizado el gráfico, no se consigue concluir firmemente con cuántos centroides debemos realizar nuestro proceso k-medias. Parece que el óptimo se encuentra entre 2 y 3.*

Examinemos ambas posibilidades:
```{r}
# 2 centroides
k2 <- kmeans(train_Kmeans_std, centers = 2, nstart = 25)

k2plot <- fviz_cluster(k2, data = train_Kmeans_std); k2plot
```
</center>\
*Aunque los datos se encuentran muy agrupados, podemos diferenciar 2 grupos. En el cúster 2 encontramos algún dato marginado. Veremos más adelante técnicas con las que determinaremos si partir el cluster o no.*

```{r}
# 3 centroides
k3 <- kmeans(train_Kmeans_std, centers = 3, nstart = 25)

k3plot <- fviz_cluster(k3, data = train_Kmeans_std); k3plot
```
</center>\
*En relación a la representación con 2 centroides, en este caso encontramos una división del grupo 2 anterior, distinguiéndose grupos 2 y 3 en este caso. La división de los datos no muestra solapamiento excesivo.*

Observemos con 7 centroides:
```{r}
# 7 centroides
k7 <- kmeans(train_Kmeans_std, centers = 7, nstart = 25)

k7plot <- fviz_cluster(k7, data = train_Kmeans_std); k7plot
```
*Encontramos mayor solapamiento que anteriormente, aunque la agrupación de los datos es sorprendentemente buena.*



### Método de la silueta.

```{r}
# Método de la silueta
fviz_nbclust(train_Kmeans_std, kmeans, method = "silhouette")
```
</center>\
*Concluimos claramente que el número óptimo de centroides es 2.*

Dividamos los datos en 2 y representemos los dos grupos.
```{r}
# Usamos k2 según la anterior gráfica
sil <- silhouette(k2$cluster, dist(train_Kmeans_std))

#Dividimos los datos
fviz_silhouette(sil)
```
</center>\
*Observamos dos grupos 1 y 2 con coeficientes 0.49 y 0.19 respectivamente* 
[NUEVO]
*También vemos que el cluster 2 presenta valores negativos, esto significa que esas obsevarciones deberían pertenecer al cluster 1. Por otro lado el cluster 1 tiene la mayoría de las observaciones por encima del coeficiente medio, mientras que el cluster 2 no tiene ninguno por encima, esto nos puede indicar que el cluster 1 está bien seleccionado. Esto puede explicar porque al utilizar más centroides lo que conseguimos es separar grupos dentro del cluster 2 y no tocamos el cluster 1.* 



### GAP.
Veamos qué resultado obtenemos con el método GAP.
```{r, warning=FALSE}

gap_stat <- clusGap(train_Kmeans_std, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)

print(gap_stat, method = "firstmax")
```
*Obtenemos que el número óptimo de centroides es 3.*

Veamos la representación.
```{r}
fviz_gap_stat(gap_stat)
```
</center>\
*Ratificamos la afirmación anterior.* 



### NbClust 

Por último, veamos cuántos grupos recomienda hacer el paquete NbClust, el cual proporciona 30 índices para determinar el número relevante de clústeres.

```{r, warning=FALSE}
# Usamos la distancia euclidea
nb <- NbClust(train_Kmeans_std, distance = "euclidean", min.nc = 2,
             max.nc = 10, method = "kmeans")
```
*La salida indica que el número de centroides óptimos es 2.*


## Clústering jerárquico

Además de los métodos empleados en secciones anteriores, en el proceso de determinación del número de clústeres óptimos se pueden emplear métodos gráficos. Concretamente, empleamos el dendrograma, una representación en forma de árbol que nos permite representar cuánto de diferentes o parecidos son los diferentes grupos.

### Clustering aglomerativo.

Representamos nuestro dendrograma empleando el método de enlace completo. Este minimiza la disimilitud máxima entre las observaciones de dos clústeres. 

```{r}
# Matriz de disimilaridades
d <- dist(train_Kmeans_std, method = "euclidean")

# Clustering jerárquico usando enlace completo
hc1 <- hclust(d, method = "complete" )

# Dendrograma
plot(hc1, cex = 0.6, hang = -1,labels = FALSE)
```
</center>\
*En este caso, se obtiene una representación con mucho agrupamiento. Observamos como cortando a una altura de 20 se obtienen en los grupos 2 y 3 datos atípicos.*

```{r}
sub_grp1<- cutree(hc1, h=20)
table(sub_grp1)
```

A continuación representamos nuestro dendrograma empleando el método de Ward, el cual minimiza la suma de las diferencias cuadradas dentro de los grupos.
```{r}
# Método de Ward
hc2 <- hclust(d, method = "ward.D2" )

# Cortamos para 2 clústeres
sub_grp2 <- cutree(hc2, k = 2)
plot(hc2, cex = 0.6,labels = FALSE)
rect.hclust(hc2, k = 2, border = 2:5)
```
</center>\
*El método Ward nos facilita una más clara representación gráfica que el método de enlace completo. Se puede observar claramente que para obtener dos grupos necesitaríamos cortar a una altura de 90.*

Veamos la cantidad de observaciones que presenta cada grupo propuesto por el método Ward.
```{r}
#Número de observaciones en cada cluster.
table(sub_grp2)
```
*El grupo 1 lo constituyen los datos contenidos en la delineación rectangular verde, mientras que los datos del grupo 2 se encuentran dentro de la roja.*

Visualicemos los dos clústeres propuestos.
```{r}
fviz_cluster(list(data=train_Kmeans_std,cluster=sub_grp2))
```
</center>\
*La visualización presenta algo de solapamiento entre algunos datos de diferentes grupos.*

Observemos para k=3.
```{r}
# Método de Ward
hc3 <- hclust(d, method = "ward.D2" )

# Cortamos para 3 clústeres
sub_grp3 <- cutree(hc3, k = 3)
plot(hc3, cex = 0.6,labels = FALSE)
rect.hclust(hc3, k = 3, border = 2:5)
```
</center>\
*Para obtener 3 grupos tendríamos que cortar a una altura de 60.*

Veamos la cantidad de observaciones que presenta cada grupo.
```{r}
#Número de observaciones en cada cluster.
table(sub_grp3)
```
*El grupo 1 lo constituyen los datos contenidos en la delineación rectangular azul, los datos del grupo 2 se encuentran dentro de la verde y los del grupo 3 en la roja.*

Visualicemos los tres clústeres propuestos.
```{r}
fviz_cluster(list(data=train_Kmeans_std,cluster=sub_grp3))
```
</center>\
*Observamos bastante solapamiento entre los 3 grupos propuestos.*

Finalizamos nuestras representaciones de clústeres jerárquicos aglomerativos con la representación del método Ward de la librería AGNES. Empleamos k=2.
```{r}
# AGNES: Agglomerative Nesting
hc4 <- agnes(train_Kmeans_std, method = "ward" )

#Cortamos para 2 clusters.
sub_grp4<-cutree(hc4,k=2)

hc4 = as.hclust(hc4)
plot(hc4, cex = 0.6, hang = -1, main = "Dendrograma de AGNES",labels = FALSE)
rect.hclust(hc4, k=2, border = 2:5)
```
</center>\
*Para obtener 2 grupos tendríamos que cortar a una altura de 90.*

Veamos la cantidad de observaciones que presenta cada grupo.
```{r}
#Número de observaciones en cada cluster.
table(sub_grp4)
```
*El grupo 1 lo constituyen los datos contenidos en la delineación rectangular roja, mientras que los datos del grupo 2 se encuentran dentro de la verde.*


Calculamos todos los coeficientes de agrupamiento de los siguientes métodos en el orden de nombramiento: Agrupamiento de enlace promedio, agrupamiento de enlace simple, agrupamiento de enlace completo, Ward.
```{r}
# Evaluación de métodos

m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

# Función para calcular el coeficiente de agrupamiento
ac <- function(x) {
  agnes(train_Kmeans_std, method = x)$ac
}

map_dbl(m, ac) 
```
*Se aprecia que el método de Ward es el que logra un mayor coeficiente aglomerativo, i.e, una muy fuerte estructura de agrupamiento.*


### Clustering divisivo
```{r}
# Clustering jerárquico divisivo
hc3 <- diana(train_Kmeans_std)

# Coeficiente de división; cantidad de estructura de agrupación encontrada
hc3$dc
```
*Nuestro alto coeficiente de división indica que se ha encontrado mucha cantidad de estructura de agrupación.* 

Representemos el dendrograma.
```{r}
# DIANA: divisive analysis.
pltree(hc3, cex = 0.6, hang = -1, main = "Dendrogram de DIANA",labels = FALSE)
```
</center>\
*Ratificamos la afirmación anterior: obtenemos unos datos muy agrupados y comprimidos. Sí se perciben diferencias significativas, debido a unas altas ramas, entre los dos primeros clústeres.* 


## Conclusión
En conclusión, podemos observar que el número de clústers óptimo sería 2 o 3. Los métodos del codo o de la silueta nos indican que debemos dividir en dos grupos y los métodos GAP y NbClust son arrojan un el mismo resultado.

Por otro lado, en el clústering aglomerativo cuando dividimos en 3 grupos observamos un notable solapamiento al representar los datos. Sin embargo, con 2 clústers parece que los resultados obtenidos son más claros. Aún así, las particiones que han realizado los diferentes dendrogramas no permiten estimar conclusiones claras por su complejidad y falta de explicabilidad.

[NUEVO]
Tras esto vemos que el método de kmedias es el que nos genera mejores clusters, ya que como se ha comentado previamente, pese a que al usar 2 clusters el segundo no recoge muy bien los datos el primero sí, y además no presenta nada de solapamiento.

En conclusión, estimamos que el número óptimo de centroides es 2: k=2.


# ML supervisado

**No se realiza un análisis discriminante lineal, puesto que previamente en el EDA se ha observado que no disponemos de datos normales.**

## Medidas de rendimiento

El dataset empleado está desbalanceado en relación al target gender. Las ediciones en un 90% de los casos son realizadas por hombres, frente al 10% restante que llevan a cabo las mujeres.

El problema de clasificación en cuestión es complejo y requiere de un modelo que, en la medida de lo posible, garantice un tasa de acierto elevada en las predicciones TP (True Positive), donde positivo sea mujer y sea cierto que la edición en Wikipedia haya sido realizada por una mujer y no por un hombre. 

Como en la mayoría de modelos de ML, el accuracy es importante para nosotros. Buscamos predecir correctamente. Sin embargo, realmente buscamos que cuando la edición sea por una mujer, seamos capaces de detectarlo en prácticamente todos los casos (alto recall), aunque el modelo sea algo más errático y prediga que ciertas ediciones por hombres hayan sido realizadas por mujeres. En este caso, el accuracy disminuiría, pero este caso es preferible a tener un accuracy superior con una tasa de FN (False positive) mayor. El modelo perfecto no existe.

Considerando lo mencionado previamente, concluimos que la medida que buscamos maximizar es el F1-Score o el F-Score generalizado, al combinar ambas precisión y recuperación. Queremos que el número de observaciones con etiqueta igual a 2 (1 hombre; 2 mujer) sean correctamente predichas. Usaremos el caso generalizado cuando queramos ponderar la precisión o recall de alguna forma concreta.

NOTA: F1-Score no tiene en cuenta el número de aciertos en la clase hombre (TN). Será importante realizar un buen valance de las medidas de rendimiento en general, especificamente de accuracy, para no obtener predicciones muy desbalanceadas con alto número de FP.


## K-Vecinos más cercanos (KNN)
Empleando el modelo KNN buscamos estimar la clase que predomina entre un número establecido de vecinos k, i.e la moda del target. Es conveniente ejecutar el algoritmo en múltiples ocasiones con distintos valores de k. Seleccionaremos aquel que minimice la tasa de errores, preservando lo máximo posible la capacidad de realizar predicciones precisas. De ahí la importancia de lograr un equilibrio.

Antes de aplicar el proceso de KNN, tenemos que modificar nuestra partición de datos en test, al igual que se ha realizado en train. 

### Modificación de test_Data

Aplicamos logaritmo a las variables a las que conviene modificar su distribución
```{r}
new_NActDays<- log10(test_Data$NActDays+0.5)
new_NPages<- log10(test_Data$NPages)
new_ns_talk<- log10(test_Data$ns_talk+0.5)
new_ns_userTalk<- log10(test_Data$ns_userTalk+0.5)
new_ns_content<- log10(test_Data$ns_content+0.5)
new_ns_user<- log10(test_Data$ns_user)
new_ns_wikipedia<- log10(test_Data$ns_wikipedia+0.5)
```

Incluimos las variables nuevas creadas a partir para comprender mejor los datos de las fechas. Además, establecemos los factores para las variables categóricas y binarizamos pagesWomen y wikiprojWomen como hicimos en nuestro conjunto train_Data. Incluimos también nuestro target.
```{r,echo=FALSE}
test_Data$ano_inicial<-as.numeric(substr(test_Data$firstDay,start = 1, stop = 4 ))
test_Data$mes_inicial<-as.numeric(substr(test_Data$firstDay,start = 5, stop = 6 ))
test_Data$dia_inicial<-as.numeric(substr(test_Data$firstDay,start = 7, stop = 8 ))
test_Data$ano_final<-as.numeric(substr(test_Data$lastDay,start = 1, stop = 4 ))
test_Data$mes_final<-as.numeric(substr(test_Data$lastDay,start = 5, stop = 6 ))
test_Data$dia_final<-as.numeric(substr(test_Data$lastDay,start = 7, stop = 8 ))
test_Data$gender<-as.factor(test_Data$gender)
test_Data$E_NEds <-as.factor(test_Data$E_NEds)
test_Data$E_Bpag <-as.factor(test_Data$E_Bpag)
test_Data$weightIJ<-as.factor(test_Data$weightIJ)
test_Data$NIJ<-as.factor(test_Data$NIJ)
test_Data$pagesWomen_bin<-ifelse(test_Data$pagesWomen>0,1,0)
test_Data$wikiprojWomen_bin<-ifelse(test_Data$wikiprojWomen>0,1,0)

delete_cat <- subset(test_Data, select=-c(gender, E_NEds, E_Bpag, firstDay, lastDay, NIJ,weightIJ,NActDays,NPages,ns_talk,ns_userTalk,ns_content,ns_user, ns_wikipedia,wikiprojWomen,pagesWomen,ano_inicial,mes_inicial,dia_inicial,ano_final,mes_final,dia_final))

df_test<- cbind(delete_cat,new_NActDays, new_NPages,new_ns_talk, new_ns_userTalk, 
                  new_ns_content,new_ns_user,new_ns_wikipedia)

#Escalamos las variables
df_kvecinos_test <- as.data.frame(scale(df_test))

# Introducimos nuestro target: gender
df_kvecinos_test$gender <- test_Data$gender
```

### Subconjunto final: df_kvecinos_train

Partimos de los datos en train_Kmeans_std puesto que son los que podemos emplear en el KNN. Añadimos nuestra variable gender, necesaria para poder realizar predicciones.
```{r}
df_kvecinos_train <- as.data.frame(train_Kmeans_std)
df_kvecinos_train["gender"] <- train_Data$gender
```


### Grid search.
Buscamos el número óptimo para el hiperparámetro k en train.

```{r}
set.seed(123)

levels(df_kvecinos_train$gender) <- c("male", "female")
levels(df_kvecinos_test$gender) <- c("male", "female")

grid <- expand.grid(.k=seq(1,30,by=1))

trainControl <- trainControl(method="repeatedcv", number=10, repeats=3,summaryFunction = prSummary, classProbs = TRUE)

fit.knn.grid <- train(gender~., data=df_kvecinos_train, method="knn",
                  metric="F" ,trControl=trainControl,tuneGrid = grid)

F1 <- with(fit.knn.grid$results, (2 * Precision * Recall) / (Precision + Recall))

best_k <- fit.knn.grid$results$k[which.max(F1)]

fit.knn.grid$bestTune<-best_k

plot(fit.knn.grid)
# Imprimimos el mejor k obtenido
print(best_k)
```
</center>\
*Se obtiene como k óptimo k=10.*


Veamos la predicción en train para k=10.
```{r, warning=FALSE}
#TRAIN

prediction.knn2_train <- predict(fit.knn.grid,newdata=df_kvecinos_train,type="prob")[,2]

clase.pred.knn2_train=ifelse(prediction.knn2_train>0.2,"female","male")

cf12 <- confusionMatrix(as.factor(clase.pred.knn2_train),as.factor(df_kvecinos_train$gender),
      positive="female", mode="everything")
print(cf12)
```
*Se obtiene un F1-Score de 0.45434.*

Veamos si en relación a test conseguimos mejores resultados con k=10.
```{r, warning=FALSE}
#TEST
prediction.knn2_test <- predict(fit.knn.grid,newdata=df_kvecinos_test,type="prob")[,2]

clase.pred.knn2_test=ifelse(prediction.knn2_test>0.2,"female","male")

cf22 <- confusionMatrix(as.factor(clase.pred.knn2_test),as.factor(df_kvecinos_test$gender),
      positive="female", mode="everything")
print(cf22)
```
*Con 10 vecinos, obtenemos un F1-Score de 0.31655, mejor que el obtenido para k=9 en test (0.26214).*

Podemos concluir que con k=10 en train el modelo se comporta de forma parecida a cómo lo hacía con k=9, mientras que en test mejora su valor de F1. El rendimiento obtenido con k=10 es superior al obtenido con k=9.



## Árboles de decisión
Inicialmente, aplicamos el método de DT al conjunto de datos en train. Incluimos tanto variables categóricas como continuas para observar desde una perspectiva general las variables que son consideradas decisivas.

```{r}
#Empleamos train_Data, con todas las variables originales y transformadas. 

#Establecemos los niveles de los factores para que funcionen correctamente las predicciones
levels(train_Data$gender) <- c("male", "female")
levels(test_Data$gender) <- c("male", "female")

fit_dt1 <- rpart(gender~., data = train_Data , method = 'class')
rpart.plot(fit_dt1, extra = 106)
```
*La variable más importante es wikiprojWomen. En segundo caso, si wikiprojWomen es 1, llegamos a un nodo terminal (hay muy pocas ediciones en proyectos 'Wiki' relacionados con la mujer). Sin embargo, si wikiprojWomen es 0, se muestra que la segunda variable más importante es NDays.*

Calculamos nuestra matriz de confusión y realizamos predicciones en train. Empleamos inicialmente el árbol generado con todas las variables. Aunque hemos confirmado que es menos interpretable, es posible que nos proporcione unos mejores resultados en relación a las predicciones.
```{r, warning=FALSE}
#TRAIN

prediction1 <- predict(fit_dt1,newdata=train_Data,type="prob") [,2] 

#Binarizamos a partir de 0.1 puesto que es lo mejor entre train y test. En train es mejor con 0.2, pero en test f1 es mucho mejor con 0.1
clase.pred.dt1=ifelse(prediction1>0.1,"female","male")

cf1 <- confusionMatrix(as.factor(clase.pred.dt1),as.factor(train_Data$gender),
      positive="female", mode="everything")
print(cf1)
```
*Binarizando a partir de 0.2 obtenemos un F1-Score de 0.42105. RECUERDA: En nuestro caso, queremos buscar maximizar el F1-Score, es decir, cuando haya algún proyecto/edición en el que participe una mujer tenemos que ser capaces de detectarlo, aunque pequemos de determinar falsos positivos en exceso (proyecto hecho por un hombre que estimamos que ha sido desarrollado por una mujer).*


Veamos cómo funciona el modelo en test.
```{r, warning=FALSE}
#TEST

prediction1_test <- predict(fit_dt1,newdata=test_Data,type="prob") [,2]

clase.pred.dt1_test=ifelse(prediction1_test>0.1,"female","male")

cf1_test <- confusionMatrix(as.factor(clase.pred.dt1_test),as.factor(test_Data$gender),
      positive="female", mode="everything")
print(cf1_test)
```
*La puntuación F1-Score se ve mermada en test.*

Atendiendo al árbol empleado para las predicciones anteriores, parece interesante binarizar la variable weightIJ, dado que tiene muchas categorías y, como está en el árbol, facilitaría su comprensión si solo tuviese 2 categorías. Sin embargo, weightIJ y NIJ son una forma de categorizar la variable NEds. 

Como NEds ya está incluida, eliminamos weightIJ y NIJ para representar el árbol. Además, sustituimos las variables originales por sus transformadas, en vez de incluirlas todas juntas. Realizamos este proceso para los datos de train y test.
```{r}
# Eliminamos variables originales, manteniendo sus respectivas transformadas. Eliminamos fechas originales.
train_dt <- subset(train_Data, select=-c(firstDay, lastDay, pagesWomen, wikiprojWomen, weightIJ, NIJ))
test_dt <- subset(test_Data, select=-c(firstDay, lastDay, pagesWomen, wikiprojWomen, weightIJ, NIJ))

#Establecemos los niveles de los factores para que funcionen correctamente las predicciones
levels(train_dt$gender) <- c("male", "female")
levels(test_dt$gender) <- c("male", "female")

fit_dt2 <- rpart(gender~., data = train_dt , method = 'class')
rpart.plot(fit_dt2, extra = 106)
```
*En este caso, como esperábamos, se emplea NEds en vez de sus hermanas categóricas weightIJ y NIJ. La comprensión del árbol es mucho mejor. Parece que el área de edición en namespace 'user' es el más importante de estas dado que juega un papel importante a la hora de fragmentar los datos.*

Realizamos la predicción con este ajuste en train.
```{r, warning=FALSE}
#TRAIN

prediction2 <- predict(fit_dt2,newdata=train_dt,type="prob") [,2]

clase.pred.dt2=ifelse(prediction2>0.1,"female","male")

cf2 <- confusionMatrix(as.factor(clase.pred.dt2),as.factor(train_dt$gender),
      positive="female", mode="everything")
print(cf2)
```
*Obtenemos resultados idénticos respecto a lo que arrojaron los modelos con los conjuntos train_Data y test_Data, por lo que las variables que hemos eliminado no afectan notablemente el rendimiento del modelo. Estas son firstDay, lastDay, pagesWomen, wikiprojWomen, weightIJ y NIJ. Nótese que en el caso de pagesWomen y wikiprojWomen, aunque estas variables originales no estén, sí se encuentras sus versiones binarizadas, por lo que, realmente, son importantes para el modelo. *

Veamos qué resultados arroja el modelo con los datos de test.
```{r, warning=FALSE}
#TEST

prediction2_test <- predict(fit_dt2,newdata=test_dt,type="prob") [,2]

clase.pred.dt2_test=ifelse(prediction2_test>0.1,"female","male")

cf2_test <- confusionMatrix(as.factor(clase.pred.dt2_test),as.factor(test_dt$gender),
      positive="female", mode="everything")
print(cf2_test)
```
*En test ocurre lo mismo que en train. Los resultados son idénticos.*

Ajustando los hiperparámemtros del modelo podemos obtener resultados específicos que puedan resultar en una mejor explicabilidad del mismo. En nuestro caso representemos el árbol con profundidad máxima y con la condición de que debe existir un número mínimo de 20 observaciones en un nodo para que se intente una división.

```{r}
# Árbol con hiperparámetros ajustados para una mayor profundidad
control <- rpart.control(minsplit=20,cp = 0, maxdepth=30)
tune.fit <- rpart(gender~., data = train_dt, method = 'class', control = control)
rpart.plot(tune.fit, extra = 106)
```
*Obtenemos un árbol mucho más profundo. Al igual que los anteriores tiene a wikiprojWomen y Ndays como variables principales.*

Veamos cómo qué resultados obtenemos con el árbol con hiperparámetros ajustados. Veamos si su alta profundidad, que involucra complejidad en su comprensión, nos permite realizar mejores predicciones.
```{r,warning=FALSE}
#TRAIN

prediction3 <- predict(tune.fit,newdata=train_dt,type="prob") [,2]

clase.pred.dt3=ifelse(prediction3>0.2,"female","male")

cf3 <- confusionMatrix(as.factor(clase.pred.dt3),as.factor(train_dt$gender),
      positive="female", mode="everything")
print(cf3)
```
*El F1-Score varía sustancialmente. En predicciones anteriores con otros árboles se obtuvieron resultados de F1-Score = 0.4 aproximadamente (en train). Con este árbol, se obtiene un valor de 0.52910.*

Veamos en test.
```{r,warning=FALSE}
#TEST
prediction3_test <- predict(tune.fit,newdata=test_dt,type="prob") [,2]

clase.pred.dt3_test=ifelse(prediction3_test>0.2,"female","male")

cf3_test <- confusionMatrix(as.factor(clase.pred.dt3_test),as.factor(test_dt$gender),
      positive="female", mode="everything")
print(cf3_test)
```
*El resultado en test no sorprende positivamente. El valor de 0.30357 no es especialmente bueno en comparación con otros árboles. Para este árbol esperábamos un valor algo mejor.*

## [NUEVO] Reajuste de hiperparámetros
Veamos qué resultado obtenemos maximizando la profundidad del árbol y ajustando algunos hiperparámetros adicionales.

```{r, warning=FALSE}
control4 <- rpart.control(minsplit=5,cp = 0, maxdepth=30)
tune.fit4 <- rpart(gender~., data = train_dt, method = 'class', control = control4)
rpart.plot(tune.fit4, extra = 106)
```

```{r, warning=FALSE}
#TRAIN & TEST
prediction4_test <- predict(tune.fit,newdata=test_dt,type="prob") [,2]

clase.pred.dt4_test=ifelse(prediction4_test>0.2,"female","male")

cf4_test <- confusionMatrix(as.factor(clase.pred.dt4_test),as.factor(test_dt$gender),
      positive="female", mode="everything")


prediction4_train <- predict(tune.fit,newdata=train_dt,type="prob") [,2]

clase.pred.dt4_train=ifelse(prediction4_train>0.2,"female","male")

cf4_train <- confusionMatrix(as.factor(clase.pred.dt4_train),as.factor(train_dt$gender),
      positive="female", mode="everything")

print(cf4_train)
print(cf4_test)
```
*Se aprecia cierto sobreajuste entre train y test, obteniéndose en train un F1-Score de 0.52910 y en test un valor de 0.30357. No se obtiene el mejor resultado para nuestro conjunto de test.*

Tras probar con diferentes ajustes de hiperparámetros, se concluye que el mejor modelo es el segundo empleado (predicción nº2), donde los valores de F1 de train a test son bastante congruentes.

## Bagging (Random Forest) 

Empleamos los mismos conjuntos de datos que para los árboles de decisión: train_dt y test_dt. Estos contienen tanto variables categoricas como continuas. Realicemos 500 árboles.
```{r}
set.seed(123) # Cada árbol se entrena con una muestra aleatoria 
rf <- randomForest(as.factor(gender)~., data=train_dt, importance=TRUE,proximity=TRUE) 
print(rf)
```
*La matriz de confusión arroja resultados coherentes y un error OOB del 10.17%, el cual no parece especialmente grande. Sin embargo, se indica un error del 80% para la predicción de mujeres. Este se calcula con los TP y los FP. Como los datos están desbalanceados, y hay muchas más observaciones hombre, el modelo en términos absolutos falla muchos más hombres que mujeres acierta.* 

*Sin embargo, si nos centramos en los valores de la columna female se observa como entre las 59 mujeres que se encuentran en train_dt, nuestro modelo acierta 43 de ellas.*

```{r}
plot(rf)
```
</center>\
*Se observa como el error para hombres (línea roja) es muy bajo. Al contrario ocurre con el error en la predicción de mujeres (línea verde), que se encuentra cercano al 80%. El error medio no es excesivamente alto, pero su valor no es lo verdaderamente importante para nosotros. Parece que el random forest no ha sido capaz de predecir correctamente con nuestros datos en train.*


Realizamos la predicción en train.
```{r, warning=FALSE}
#TRAIN

prediction.rf_train <- predict(rf,newdata=train_dt,type="prob") [,2]

clase.pred.rf_train=ifelse(prediction.rf_train>0.2,"female","male") # con 0.2 o 0.3 se obtiene parecido. Con 0.4 sale F1=1 Pero lo dejamos con 0.2 al ser lo mejor en test y en train obtener resultados buenísimos también.

cf.rf_train <- confusionMatrix(as.factor(clase.pred.rf_train),as.factor(train_dt$gender),
      positive="female", mode="everything")
print(cf.rf_train)
```
*El modelo acierta en prácticamente todos los casos. F1-Score=0.9755. En caso de binarizar con 0.4 obtenemos un F1-Score=1, aunque ese umbral de binarización probablemente cause sobreajuste en el modelo, obteniéndose unos futuros resultados peores en test.*


Realicemos la predicción en test. Véamos si el modelo está sobreajustando.
```{r, warning=FALSE}
#TEST

prediction.rf_test <- predict(rf,newdata=test_dt,type="prob") [,2]

clase.pred.rf_test=ifelse(prediction.rf_test>0.2,"female","male")

cf.rf_test <- confusionMatrix(as.factor(clase.pred.rf_test),as.factor(test_dt$gender),
      positive="female", mode="everything")
print(cf.rf_test)
```
*El valor de F1-Score de 0.36257 es realmente bueno en comparación con lo obtenido con otros modelos en conjuntos de test. Sin embargo, en comparación con los resultados de train, el modelo presenta sobreajuste.*

Además, es conveniente observar qué importancia otorga el modelo a las diferentes variables.
```{r}
importance(rf)
```
*Con mucha diferencia, así como nos indicaban los árboles de decisión que generamos previamente de forma individual, la variable más decisiva para realizar predicciones para el modelo de Random Forest son wikiprojWomen_bin y NActDays, seguidas por NPages y NDays. Destaca también para la predicción de "male" la variable ns_content.*

Sí es coherente que la variable wikiprojWomen (número de ediciones en proyectos 'Wiki' relacionados con la mujer) juegue un papel importante en los modelos de predicción, al tener una fuerte relación con la mujer y su implicación en las ediciones de Wikipedia. 

Si sumamos Ndays (número de días con ediciones), que nos proporciona información sobre la cantidad de días que ha estado editando una persona, y NPages (número de páginas diferentes editadas) podemos predecir especialmente para las mujeres en base al timepo empleado y su rendimiento en la redacción de páginas diferentes.


```{r}
varImpPlot(rf)
```
</center>\
*Se observa que con los distintos métodos la variable NDays toma una posición alta en la escala de importancia para la predicción.*


## [NUEVO] Reajuste de hiperparámetros.
Realicemos una nueva aproximación sobre modelo empleando distintos hiperparámetros para ver si conseguimos reducir el sobreajuste.
```{r}
rf2 <- randomForest(as.factor(gender)~., data=train_dt, ntree=1800, mtry=20, nodesize=80, 
                    maxnodes=50, importance=TRUE,proximity=TRUE) 
print(rf2)
```
*Obtenemos un error ligeramente inferior en relación al modelo inicial para las mujeres. Para los hombres aumenta muy ligeramente. El valor de OOB es similar.*

```{r,warning=FALSE}
#TRAIN & TEST
prediction.rf2_train <- predict(rf2,newdata=train_dt,type="prob") [,2]

clase.pred.rf2_train=ifelse(prediction.rf2_train>0.2,"female","male") # con 0.2 o 0.3 se obtiene parecido. Con 0.4 sale F1=1 Pero lo dejamos con 0.2 al ser lo mejor en test y en train obtener resultados buenísimos también.

cf.rf2_train <- confusionMatrix(as.factor(clase.pred.rf2_train),as.factor(train_dt$gender),
                               positive="female", mode="everything")


prediction.rf2_test <- predict(rf2,newdata=test_dt,type="prob") [,2]

clase.pred.rf2_test=ifelse(prediction.rf2_test>0.2,"female","male")

cf.rf2_test <- confusionMatrix(as.factor(clase.pred.rf2_test),as.factor(test_dt$gender),
                              positive="female", mode="everything")
print(cf.rf2_train)

print(cf.rf2_test)
```
*En train se obtiene un valor de F1-Score de 0.49596 y en test de 0.32203. Este modelo ya no presenta el sobreajuste que presentaba el modelo anterior. El modelo anterior presentaba un valor de F1 en test de 0.36 aproximadamente, lo cual es superior al obtenido con este modelo. También es cierto, que con este modelo esperamos generalizar mejor.*


## Boosting (XGBoost).

Empleamos el ensamblado de boosting como alternativa al baging anterior. En este caso buscamos mejorar iterativamente un modelo inicialmente sencillo. 
Nótese que para el correcto funcionamiento del modelo XGBoost se necesitan definir unas funciones específicas.

```{r}
# Transformación previa de datos

train_xg <- map_df(train_dt, function(columna) {
  columna %>% 
    as.factor() %>% 
    as.numeric %>% 
    { . - 1 }
})

datos<- list()
datos$train <- train_xg

test_xg <- map_df(test_dt, function(columna) {
  columna %>% 
    as.factor() %>% 
    as.numeric %>% 
    { . - 1 }
})

datos$test <- test_xg

# Convertimos a formato Dmatrix
datos$train_mat  <- subset(datos$train, select=-c(gender)) %>% 
  as.matrix() %>% 
  xgb.DMatrix(data = ., label = datos$train$gender)

datos$test_mat  <- subset(datos$test, select=-c(gender)) %>% 
  as.matrix() %>% 
  xgb.DMatrix(data = ., label = datos$test$gender)
```

Una vez hemos realizado el procesamiento de datos pertinente, entrenamos el modelo con nuestra matriz de datos en train.
```{r}

datos$modelo1 <- xgboost(data = datos$train_mat,  
                 objective = "binary:logistic", 
                  nround = 10,  
                  max_depth=2, # número de nodos de bifurcación de los dt en train 
                  eta =0.9, # tasa de aprendizaje
                  nthread =2) 
```
 *Observamos que el error incial toma un valor aproximado de 0.35. Finalmente, al alcanzar la iteración 10, el error se reduce a un valor cercano a 0.27. Ajustando la tasa de aprendizaje a 0.9 se ha conseguido minimizar el error lo máximo posible.*


```{r}
# Visualización del conjunto de árboles como una única unidad 
xgb.plot.multi.trees(model = datos$modelo1)
```
*Según nuestro modelo XGBoost, la variable con mayor calidad (mayor importancia en los árboles) es NDays, mientras que la segunda es wikiprojWomen. Sin embargo, wikiprojWomen se encuentra en una posición jerárquica superior en el árbol. Además, como también ocurría con el modelo RF, NPages tiene una relevancia considerable en el modelo. Cabe destacar la importancia de la variable año inicial generada por nosotros durante el proceso EDA. El año que cada individuo empieza a realizar ediciones en wikipedia tiene relevancia en el modelo al estar en el nodo raíz.* 

Realizamos un gráfico para visualizar la importancia de cada variable en el modelo en train.
```{r}
importance_matrix <- xgb.importance(model = datos$modelo1)
xgb.plot.importance(importance_matrix)
```
*Ratificamos los comentarios anteriores.*

Como los nodo hoja arrojan probabilidades a las que se les ha aplicado el logaritmo, convertimos los valores para obtener las probabilidades reales de los nodos hoja.
```{r}
# Función que convierte los valores proporcionados por los nodos hoja
odds_to_probs <- function(odds){
    return(exp(odds)/ (1 + exp(odds)))
}
```

```{r}

hoja1<-odds_to_probs(-0.79673); hoja1
hoja2<-odds_to_probs(1.8144); hoja2
hoja3<-odds_to_probs(-2.7018); hoja3
hoja4<-odds_to_probs(0.42505); hoja4

```
Cabe destacar la hoja 2. Si una observación acabó en la hoja2 (1.8144), la probabilidad media de que una observación fuera “female” en la variable gender es del 86%. 

Si nos fijamos en las variables con mayor importancia que se encuentran en a lo largo del camino del árbol hasta llegar a dicha hoja, se observa que las que más contribuyen a que el modelo prediga "female" son wikiprojWomen, NDays, NPages, Año_inicial y ns_wikipedia. 

Veamos como predice nuestro modelo de XGBoost en train.
```{r}
# TRAIN

datos$predict_train <- predict(datos$modelo1, datos$train_mat)
cbind(datos$predict_train > 0.3, datos$train$gender) %>% 
  data.frame() %>% 
  table() %>% 
  confusionMatrix(positive="1", mode="everything")

```
*Obtenemos un valor de F1-Score de 0.48831, que es bueno para lo obtenido con otros modelos. Parece que con estos datos es complicado acertar con seguridad al predecir si el género será femenino. Un valor de 0.48831 es positivo.*


Por último, veamos qué predicciones arroja XGBoost para nuestra matriz en test.
```{r}
# TEST
datos$predict_test <- predict(datos$modelo1, datos$test_mat)
cbind(datos$predict_test > 0.3, datos$test$gender) %>% 
  data.frame() %>% 
  table() %>% 
  confusionMatrix(positive="1", mode="everything")
```
*Se obtiene un F1-Score de 0.37037. En test es el F1-Score que mejor resultado presenta. Parece que este modelo funciona adecuadamente con nuestros datos y nuestro objetivo de maximizar la medida F1.*



## Clasificador Naive Bayes. 
El clasificador Naive Bayes puede ser de gran utilidad en este dataset, dado el gran número de variables de entrada que presenta y los amplios dominios en los que están definidas. Sin embargo, para variables cuantitativas se asume la distribución normal y sabemos que nuestros datos distan notoriamente de ser normales. Los resultados pueden no ser tan altos como en modelos anteriores puesto que Naive Bayes es mucho más simple y reducido que el resto de los modelos empleados.

Usamos los conjuntos de datos que empleamos anteriormente en los árboles de decisión puesto que incluyen variables continuas y categóricas. Obtengamos el modelo y realicemos la predicción en train.

```{r}
#TRAIN
model_bayes <- naive_bayes(as.factor(gender) ~ ., data = train_dt, usekernel = T) 
model_bayes$prior
```
*Estas son las probabilidades a priori con las que trabajamos para clasificar los datos.*

Aunque ajustemos hiperparámetros como "laplace" para añadir suavidad en la clasificación, los resultados no varían en exceso. No se incluye un modelo con distintos hiperparámetros puesto que no se han obtenido mejores resultados que reduzcan el leve sobreajuste que presenta este modelo.

Predicción en train:
```{r, warning=FALSE}
#TRAIN

prediction.bayes <- predict(model_bayes,newdata=train_dt,type="prob")[,2]

clase.pred.bayes=ifelse(prediction.bayes>0.2,"female","male") 

cf.bayes <- confusionMatrix(as.factor(clase.pred.bayes),as.factor(train_dt$gender),
      positive="female", mode="everything")
print(cf.bayes)

```
*El modelo arroja un valor para el F1-Score muy bueno: 0.39306. Parece que la precisión del modelo no es tan buena (0.34000), considerando que hay más mujeres que predice como hombre que como mujer y que hay más hombres que predice como mujer que mujeres que predice como mujer. Esto es algo no tan sorprendente dado el gran desbalanceo que presentan nuestros datos y las probabilidades a priori obtenidas.*

```{r, warning=FALSE}
#TEST

prediction.bayes_test <- predict(model_bayes,newdata=test_dt,type="prob")[,2]

clase.pred.bayes=ifelse(prediction.bayes_test>0.2,"female","male") 

cf.bayes_test <- confusionMatrix(as.factor(clase.pred.bayes),as.factor(test_dt$gender),
      positive="female", mode="everything")
print(cf.bayes_test)
```
*Aunque es cierto que se tenía una previsión de que el modelo Naive Bayes iba a arrojar buenos resultados, parece que la falta de normalidad de nuestros datos ha causado que mermen los valores asociados a las medidas de rendimiento. El valor para el F1-Score de 0.27848. Quizás es algo peor de lo esperado.*



# Comparación de modelos.
Cada modelo de los empleados funciona de una forma. Algunos necesitan parámetros de entrada o emplean datos aleatorios... Cada uno otorga su propia matriz de confusión. Sería útil poder compararlos a todos simultáneamente de alguna forma.

Para ello, representamos la curva ROC, donde comparamos el rendimiento de cada modelo con el conjunto de datos en test.

```{r}

par(pty = "s") 

#XGBoost
roc(test_dt$gender, datos$predict_test,plot=TRUE, legacy.axes = TRUE,
    percent = TRUE, xlab = "Porcentaje Falsos positivos",
    ylab = "Porcentaje verdaderos postivios", col = "#377eb8", lwd = 2,
     print.auc = TRUE,legend=TRUE,  brier.in.legend =TRUE)

# Decision tree
roc(test_dt$gender, prediction2_test, percent=TRUE, col="goldenrod",lwd= 2,
    print.auc =TRUE, add=TRUE,print.auc.y = 30,plot=TRUE,legend=TRUE)

# Random Forest
roc(test_dt$gender, prediction.rf_test, percent=TRUE, col="salmon",lwd= 2,
    print.auc =TRUE, add=TRUE,print.auc.y = 20,plot=TRUE,legend=TRUE)

# KNN
roc(df_kvecinos_test$gender, prediction.knn2_test, percent=TRUE, col="#977eb8",lwd= 2,
    print.auc =TRUE, add=TRUE,print.auc.y = 10,plot=TRUE,legend=TRUE)

# Naive Bayes
roc(test_dt$gender, prediction.bayes_test, percent=TRUE, col="#4daf4a",lwd= 2,
         print.auc =TRUE, add=TRUE,print.auc.y = 40,plot=TRUE,legend=TRUE)

legend("bottom",
       legend=c("XGBOOST", "DT", "RF","KNN","NAIVE"),
       col=c("#377eb8", "goldenrod","salmon","#977eb8","#4daf4a"),
       lwd=2, cex =.5, xpd = TRUE, horiz = TRUE)
```
</center>\

En primer lugar, una vez obtenida nuestra gráfica con las diferentes curvas ROC asociadas a los modelos, podemos concluir que, en términos de área bajo la curva, el modelo que mejor predicciones arroja sobre nuestros datos es el XGBoost (AUC=72.1). Con este modelo, el valor del F1-Score obtenido en train es de 0.48831. Este valor no es el mejor que hemos obtenido con la partición de train; sin embargo, el modelo XGBoost generaliza muy bien para otros datos, lo cual se refleja en el valor de la medida de rendimiento F1-Score: 0.37037. Puede parecer un valor relativamente bajo, pero es el más alto que hemos obtenido en test para los diferentes modelos empleados.

En segundo lugar, encontramos el modelo Naive Bayes. Desde un inicio, las expectativas eran altas con este modelo. En términos generales, este tipo de modelos son notablemente útiles prediciendo en problemas con muchas variables, especialmente con variables cuantitativas con un alto número de posibles valores. Aún así, se asume la normalidad de las variables categóricas y las nuestras no lo son.

La predicción en train fue la peor, con un valor de F1-Score de 0.39306, y la de test tampoco fue buena, con un valor de 0.27848. El modelo es más simple que los anteriores, por lo que es de esperar observar un peor rendimiento, aunque no parece ser la mejor opción para maximizar nuestro F1. Se puede justificar un AUC muy alto porque al representar la curva ROC se enfrentan el recall y la specifity, y esta última toma un valor alto de 0.88036 para este modelo "Naive". 

En tercer lugar, no muy lejos del modelo XGBoost, se encuentra el Random Forest. El modelo de Decision Tree por sí mismo, ha obtenido los peores resultados en términos de AUC. Sin embargo, al realizar el modelo ensamblado de Random Forest, la suma de todos los árboles nos ha proporcionado un incremento del AUC de alrededor de un 10% con respecto al DT.

Con Random Forest se obtiene el mejor valor de F1-Score en train, siendo este de 0.9755. El algoritmo de clasificación arroja una matriz de confusión ideal y el comportamiento en test es bueno. Claramente, el valor de F1-Score en test (0.36257) dista mucho del valor de F1-Score en train, pero es prácticamente idéntico al que otorga el XGBoost, un modelo más complicado y complejo de entender.

Finalmente, el modelo de K-Nearest Neighbours presenta un valor de AUC algo inferior del 70%. La diferencia con el AUC de los tres mejores modelos no es gigante, por lo que no descartamos la posibilidad de escogerlo como modelo final. En el caso del modelo Decision Tree, las diferencias con los mencionados anteriormente son significativas. Descartamos este modelo. 

Estos son los valores de F1-Score obtenidos con los diferentes modelos en train y test respectivamente.

            
XGBoost:	0.48831		  0.37037

DT: 			0.40000			0.32168

RF: 			0.9755  		0.36257

KNN:			0.45434		  0.31655

Naive:		0.39306			0.27848

**Por tanto, teniendo en consideración todo lo analizado con anterioridad, la calidad de interpretabilidad y la complejidad de los distintos modelos, podemos concluir que el mejor modelo es el Random Forest.** En nuestro caso, escogeríamos el modelo RF para predecir con los datos de validación, puesto que el AUC obtenido es prácticamente el más alto (el AUC del RF dista en 0.6 del AUC del XGB). Además, el modelo emplea árboles de decisión que son más sencillos de interpretar que el modelo que lidera la lista de valores AUC. 

# Rendimiento en partición de validación.
Una vez seleccionado el modelo tras su evaluación en train y test, obtengamos sus medidas de rendimiento para la partición de validación. Antes de nada, realicemos las transformaciones oportunas en la partición de validación, así como hicimos en test, para que la predicción y los nuevos datos tengan dimensiones equilibradas.

```{r}
new_NActDays<- log10(val_Data$NActDays+0.5)
new_NPages<- log10(val_Data$NPages)
new_ns_talk<- log10(val_Data$ns_talk+0.5)
new_ns_userTalk<- log10(val_Data$ns_userTalk+0.5)
new_ns_content<- log10(val_Data$ns_content+0.5)
new_ns_user<- log10(val_Data$ns_user)
new_ns_wikipedia<- log10(val_Data$ns_wikipedia+0.5)
```

Incluimos las variables nuevas creadas a partir para comprender mejor los datos de las fechas. Además, establecemos los factores para las variables categóricas y binarizamos pagesWomen y wikiprojWomen como hicimos en nuestro conjunto train_Data. Incluimos también nuestro target.
```{r,echo=FALSE}
val_Data$ano_inicial<-as.numeric(substr(val_Data$firstDay,start = 1, stop = 4 ))
val_Data$mes_inicial<-as.numeric(substr(val_Data$firstDay,start = 5, stop = 6 ))
val_Data$dia_inicial<-as.numeric(substr(val_Data$firstDay,start = 7, stop = 8 ))
val_Data$ano_final<-as.numeric(substr(val_Data$lastDay,start = 1, stop = 4 ))
val_Data$mes_final<-as.numeric(substr(val_Data$lastDay,start = 5, stop = 6 ))
val_Data$dia_final<-as.numeric(substr(val_Data$lastDay,start = 7, stop = 8 ))
val_Data$gender<-as.factor(val_Data$gender)
val_Data$E_NEds <-as.factor(val_Data$E_NEds)
val_Data$E_Bpag <-as.factor(val_Data$E_Bpag)
val_Data$weightIJ<-as.factor(val_Data$weightIJ)
val_Data$NIJ<-as.factor(val_Data$NIJ)
val_Data$pagesWomen_bin<-ifelse(val_Data$pagesWomen>0,1,0)
val_Data$wikiprojWomen_bin<-ifelse(val_Data$wikiprojWomen>0,1,0)

delete_cat <- subset(val_Data, select=-c(E_NEds, E_Bpag, firstDay, lastDay, NIJ,weightIJ,NActDays,NPages,ns_talk,ns_userTalk,ns_content,ns_user, ns_wikipedia,wikiprojWomen,pagesWomen,ano_inicial,mes_inicial,dia_inicial,ano_final,mes_final,dia_final))

df_val<- cbind(delete_cat,new_NActDays, new_NPages,new_ns_talk, new_ns_userTalk, 
                  new_ns_content,new_ns_user,new_ns_wikipedia)
```

Ahora, consideremos las mismas variables que hemos introducido a nuestro modelo de Random Forest en las particiones de train y test.

```{r}
val_dt <- subset(val_Data, select=-c(firstDay, lastDay, pagesWomen, wikiprojWomen, weightIJ, NIJ))
levels(val_dt$gender) <- c("male", "female")
```

## Predicción con modelo.
Veamos cómo predice el modelo entrenado en train para nuestros datos de validación.
```{r, warning=FALSE}
#VALIDATION1

prediction_val <- predict(rf,newdata=val_dt,type="prob") [,2]

clase.pred.dt_val=ifelse(prediction_val>0.2,"female","male")

cf_val <- confusionMatrix(as.factor(clase.pred.dt_val),as.factor(val_dt$gender),
      positive="female", mode="everything")
print(cf_val)
```
*El valor de F1-Score obtenido es de 0.42697. Como ocurre con todos los modelos en este dataset, no se obtienen valores de las medidas de rendimiento muy altas. Sin embargo, este valor supera al obtenido en test (0.36257). Los datos son complejos y los modelos encuentran complicado predecir con precisión (precision=0.3766). Un valor como el obtenido en validación es bueno en comparación con los obtenidos previamente.*

## [NUEVO] Predicción 2 con RF modificado.
Probemos ahora con el modelo entrenado tras la primera revisión de la memoria. Este presenta un valor de F1 para test algo peor que el modelo anterior. Sin embargo, el sobreajuste se ha reducido en su mayoría.

```{r, warning=FALSE}
#VALIDATION2

prediction_val2 <- predict(rf2,newdata=val_dt,type="prob") [,2]

clase.pred.dt_val2=ifelse(prediction_val2>0.2,"female","male")

cf_val2 <- confusionMatrix(as.factor(clase.pred.dt_val2),as.factor(val_dt$gender),
      positive="female", mode="everything")
print(cf_val2)
```
*Obtenemos un valor de F1-Score algo superior al obtenido anteriormente aunque bastante similar: 0.43697. Sin embargo, con este modelo estamos seguros de que no se está sobreajustando, puesto que el valor obtenido para train y test es de 0.50794 y 0.31148 respectivamente.*


# Conclusión final.
Como conclusión hemos podido comprobar que a la hora de trabajar con datos reales reales las distribuciones que estos tienen en su gran mayoría no son ideales y ha habido que transformarlas. Con estos cambios se pierde explicabilidad, aunque se vuelve mucho más sencillo trabajar con las variables y sus distribuciones. 

Después de haber tratado con estas variables, nos hemos percatado de que en ocasiones resulta de gran utilidad crear variables a partir de otras. Nuestro conjunto de datos original ya presentaba una categorización de otra variable continua. Además, nosotros hemos creado otras variables, separando una variable de tipo fecha en tres variables distintas con información del día, mes o año. Alguna de estas nuevas variables ha presentado notable relevancia en el entrenamiento de distintos modelos.

Otro reto importante que nos ha planteado este conjunto de datos recae en la alta agrupación que presentan las distribuciones de las variables. Procedimientos como el PCA o algunos métodos de Aprendizaje no Supervisado no han ofrecido conclusiones precisas ni contundentes. Aún habiendo transformado algunas variables, no se han podido diferenciar agrupaciones de datos claras o visualizar correlaciones fuertes entre variables en las componentes principales. 

En relación al rendimiento de los modelos, se ha detectado la importancia de realizar un buen ajuste de los hiperparámetros, ya que puede incrementar notablemente su calidad de predicción. Asimismo, a la hora de escoger un modelo final, es tan importante su rendimiento como su interpretabilidad: a pesar de que un modelo pueda ser mejor que otro en términos de F1-Score, si no se es capaz de explicar a la persona beneficiaria del análisis el esfuerzo habrá sido en vano. La mayoría del tiempo del proyecto está dedicado a esto mismo: entender los datos con los que se trabaja.

# Trabajo a futuro.
La extensión de un trabajo de estas características, considerando la complejidad de los datos del estudio y la incontable cantidad de modelos que se podrían emplear con estos, puede alargarse largos periodos de tiempo. Considerando nuestro conocimiento y franja temporal de trabajo, hemos concluido que existen diversos cambios o mejoras que podrían realizarse en un futuro mediante una nueva aproximación del conjunto de datos.

Lo primero que se nos viene a la mente es utilizar datos de wikipedia de otros países para contrastar con los nuestros de España. Bien podríamos evaluar nuestro modelo con otros datos, o incluso realizar un análisis breve de cómo está repartido el género en estos nuevos países.

Por otro lado, se podría llegar a probar con distintos tamaños de partición y ver si este cambio en las particiones de los datos podría potencialmente afectar al rendimiento de los modelos o incluso cambiar nuestra estimación del mejor modelo a seleccionar. 

Ya que hemos hablado de la importancia de los hiperparámetros en los modelos, en un futuro podemos modificar los hiperparámetros de los árboles de decisión e incluso probar con otros tipos de árboles distintos y ver el rendimiento de estos. Sería conveniente intentar reducir el sobreajuste de modelos como el Random Forest, aumentando lo máximo posible su calidad de predicción en test.

Por último, en caso de disponer de más tiempo, podría realizarse un entrenamiento de los modelos con otros subconjuntos de variables: emplear variables categóricas en lugar de las continuas o utilizar un mayor porcentaje del total. Este cambio supondría un largo proceso y probablemente se perdería explicabilidad. A cambio, quizás se pueda potenciar el rendimiento de los modelos. Una vez hecho habría que valorar si ese rendimiento es lo suficientemente significativo, considerando la pérdida de explicabilidad, e intentar hallar una forma de mejorar en ese aspecto.


# [NUEVO] Nuevas tendencias.

## Explicabilidad de la toma de decisiones.
Busquemos obtener una explicación con transparencia de las decisiones tomadas por el modelo para finalmente arrojar las predicciones que indica. Se usa la técnica SHAP. **Nótese que el importante desbalanceo contemplado en el dataset, en términos de género, causa que la mayoría de las predicciones sean masculinas.**

Empleamos el modelo que seleccionamos para validación: rf2.
```{r}
# Primera observación (quitamos target)
obs_test_dt1 <- test_dt[1,-1]
obs_test_dt1
```
*Trabajaremos con esta observación. La primera del conjunto de datos.*

```{r}
explain.rf <- DALEX::explain(model=rf2,data=test_dt[,-1],y=test_dt[,1]=="female",label="Random Forest")
```

```{r}
predict(explain.rf, obs_test_dt1)
```
*La predicción para la primera observación es 'male'. Para que fuese 'female', la probabilidad debería superar el valor 0,5.*


Veamos los valores Shapley.
```{r}
shap1.new <- predict_parts(explainer = explain.rf, 
                      new_observation = obs_test_dt1, 
                      type = "shap",B = 25)

plot(shap1.new,show_boxplots=FALSE)
```
</center>\
*Las variables principalmente asociadas a la predicción de género masculino son NActDays, número de días con ediciones, y NDays, número de días totales. Las variables que indican que la respuesta predicha debería ser 'female' son NPages y ns_user, número de ediciones con namespace 'usuario'.*

Escogemos otra observación y observamos de nuevo los valores Shapley.

```{r}
# Observación 100
obs_test_dt2 <- test_dt[500,-1]

predict(explain.rf, obs_test_dt2)
```
*La predicción para la observación es 'female'.*


```{r}
shap2.new <- predict_parts(explainer = explain.rf, 
                      new_observation = obs_test_dt2, 
                      type = "shap",B = 20)

plot(shap2.new,show_boxplots=FALSE)
```
</center>\
*Las variables más decisivas para esta predicción femenina son NPages, NDays y NActDays. Sin embargo, el valor de 0 en la variable wikiprojWomen_bin indica que la respuesta es 'male': la mayoría de individuos con wikiprojWomen_bin=0 son hombres, mientras que la mayoría de observaciones con wikiprojWomen_bin=1 son mujeres.*

Veamos qué se obtiene para las 629 observaciones de test_dt. Lo normal es encontrar una mayoría de predicciones FALSE.
```{r}
# Todas las observaciones: FALSE --> male ;  TRUE -> female
predict(explain.rf, test_dt[,-1])>0.5
```
Por ejemplo, la predicción para la observación 24 es 'female'. Véamos para esta predicción femenina qué variables se han considerado principalmente.

```{r}
# Observación 24
obs_test_dt3 <- test_dt[24,-1]

predict(explain.rf, obs_test_dt3)
```

```{r}
shap3.new <- predict_parts(explainer = explain.rf, 
                      new_observation = obs_test_dt3, 
                      type = "shap",B = 10)

plot(shap3.new,show_boxplots=FALSE)
```
</center>\
*Las variables más importantes para el modelo Random Forest han sido NPages, NDays y NActDays, todas con el valor 3 (bajo para las distribuciones que presentan). Han afectado negativamente a la predicción de female las variables mes_inicial con valor 2 y año_final con valor 2017. Puede ser que en febrero de 2017 haya habido más incorporaciones a las ediciones de hombres que de mujeres.*


## Contrafácticos

*Buscamos que variables se podrían cambiar para que la predicción cambie.*
```{r}
set.seed(123)
#Entrenamos el modelo quitando la observación que usaremos para los contrafácticos
# Cargar tus datos y entrenar tu modelo (reemplaza esto con tus propios datos y modelo)
train_dt$pagesWomen_bin<-as.factor(train_dt$pagesWomen_bin)
train_dt$wikiprojWomen_bin<-as.factor(train_dt$wikiprojWomen_bin)
rf <- randomForest(as.factor(gender) ~ ., data = train_dt[-1,], importance = TRUE, proximity = TRUE, type = TRUE)

predictor = iml::Predictor$new(rf,type = "prob")

obs_interes = train_dt[1,]
predictor$predict(obs_interes) #Probabilidad de que sea mujer del 4.6%

#Veamos que cambiar para aumentar eso
moc_classif = MOCClassif$new(predictor,epsilon = 0)
subset_to_valid = moc_classif$find_counterfactuals(
   obs_interes, desired_class = "female", desired_prob = c(0.6, 1))


```
```{r}
print(subset_to_valid)

head(subset_to_valid$predict(), 3)


```
```{r}
nrow(subset_to_valid$data)

head(subset_to_valid$data,3)
```
```{r}
subset_to_valid$plot_freq_of_feature_changes(subset_zero = TRUE)
```
```{r}

subset_to_valid$plot_parallel(feature_names = names(
  subset_to_valid$get_freq_of_feature_changes()), digits_min_max = 2L)
```
*Podemos concluir que las variables que si aumenta el valor de las variables, excepto las fechas, que es en ese caso deben disminuir, sería mas fácil predecir que fuera mujer. Y como era de esperar la variable que más afecta su cambio es wikiprojWomen_bin, la cual pasa de valer 0 a valer 1. Con estos cambios, una predicción pasaría a tener un 60% de probabilidad de ser mujer*



# [NUEVO] Aprendizaje Activo

Muchas gracias por su tiempo y atención. 